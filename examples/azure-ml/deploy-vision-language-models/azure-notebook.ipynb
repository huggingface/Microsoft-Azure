{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa3b3fb3-ccce-4654-8a3a-02bb4016eeaf",
   "metadata": {},
   "source": [
    "# Deploy Vision Language Models (VLMs) on Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975fe589-9800-4124-a3be-d9f618da61a4",
   "metadata": {},
   "source": [
    "This example showcases how to deploy a Vision Language Model (VLM), i.e., a Large Language Model (LLM) with vision understanding, from the Hugging Face Collection on Azure ML as a Managed Online Endpoint, powered by Hugging Face's Text Generation Inference (TGI). Additionally, this example also showcases how to run inference with both the Azure Python SDK, OpenAI Python SDK, and even how to locally run a Gradio application for chat completion with images.\n",
    " \n",
    "> [!NOTE]\n",
    "> Note that this example will go through the Python SDK / Azure CLI programmatic deployment, if you'd rather prefer using the one-click deployment experience, please check [One-click Deployment on Azure ML](https://huggingface.co/docs/microsoft-azure/guides/one-click-deployment-azure-ml).\n",
    "\n",
    "TL;DR Text Generation Inference (TGI) is a solution developed by Hugging Face for deploying and serving LLMs and VLMs with high performance text generation. Azure Machine Learning is a cloud service for accelerating and managing the machine learning (ML) project lifecycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d723dca0-9016-4b7c-a21b-ff01f75ea440",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef145992-e426-491d-b218-87ecd9d06d65",
   "metadata": {},
   "source": [
    "This example will specifically deploy [`Qwen/Qwen2.5-VL-32B-Instruct`](https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct) from the Hugging Face Hub (or see [Qwen2.5-VL-32B-Instruct page on AzureML](https://ml.azure.com/models/qwen-qwen2.5-vl-32b-instruct/version/1/catalog/registry/HuggingFace)) as an Azure ML Managed Online Endpoint, as it's one of the latest VLMs from Qwen, released after the impact and feedback from the previous Qwen2 VL release, with some key enhancements such as:\n",
    "\n",
    "- **Understand things visually**: Qwen2.5-VL is not only proficient in recognizing common objects such as flowers, birds, fish, and insects, but it is highly capable of analyzing texts, charts, icons, graphics, and layouts within images.\n",
    "- **Being agentic**: Qwen2.5-VL directly plays as a visual agent that can reason and dynamically direct tools, which is capable of computer use and phone use.\n",
    "- **Understanding long videos and capturing events**: Qwen2.5-VL can comprehend videos of over 1 hour, and this time it has a new ability of cpaturing event by pinpointing the relevant video segments.\n",
    "- **Capable of visual localization in different formats**: Qwen2.5-VL can accurately localize objects in an image by generating bounding boxes or points, and it can provide stable JSON outputs for coordinates and attributes.\n",
    "- **Generating structured outputs**: for data like scans of invoices, forms, tables, etc. Qwen2.5-VL supports structured outputs of their contents, benefiting usages in finance, commerce, etc.\n",
    "\n",
    "![Qwen2.5 VL 32B Instruct on the Hugging Face Hub](./qwen2.5-vl-hub.png)\n",
    "\n",
    "![Qwen2.5 VL 32B Instruct on Azure ML](./qwen2.5-vl-azure-ml.png)\n",
    "\n",
    "For more information, make sure to check [their model card on the Hugging Face Hub](https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct/tree/main/README.md).\n",
    "\n",
    "> [!NOTE]\n",
    "> Note that you can select any LLM available on the Hugging Face Hub with the \"Deploy to AzureML\" option enabled, or directly select any of the LLMs available in the Azure ML Model Catalog under the \"HuggingFace\" collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763a3a3a-aae1-43be-a925-0a4e1f5493a3",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037b80f6-10c2-4f23-b035-9e74a669c7d9",
   "metadata": {},
   "source": [
    "To run the following example, you will need to comply with the following pre-requisites, alternatively, you can also read more about those in the [Azure Machine Learning Tutorial: Create resources you need to get started](https://learn.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources?view=azureml-api-2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d202cf55-91bc-4f45-b97d-845c3ad8488a",
   "metadata": {},
   "source": [
    "### Azure Account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96373e3-8e3b-49dc-a423-719d7139f5eb",
   "metadata": {},
   "source": [
    "A Microsoft Azure account with an active subscription. If you don't have a Microsoft Azure account, you can now [create one for free](https://azure.microsoft.com/en-us/pricing/purchase-options/azure-account), including 200 USD worth of credits to use within the next 30 days after the account creation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35caf6b-512f-4b82-894c-f3508b5857be",
   "metadata": {},
   "source": [
    "### Azure CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1c8888-537f-47e6-b4b3-2aaccdefe9e6",
   "metadata": {},
   "source": [
    "The Azure CLI (`az`) installed on the instance that you're running this example on, see [the installation steps](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest), and follow the steps of the prefered method based on your instance. Then log in into your subscription as follows:\n",
    "\n",
    "```bash\n",
    "az login\n",
    "```\n",
    "\n",
    "More information at [Sign in with Azure CLI - Login and Authentication](https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli?view=azure-cli-latest)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4967c565-d6bc-469f-9310-2771e188fd8e",
   "metadata": {},
   "source": [
    "### Azure Resource Group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57e94a0-b247-44bc-a78d-7ceadb1deb85",
   "metadata": {},
   "source": [
    "An Azure Resource Group under the one you will create the Azure ML workspace and the rest of the required resources. If you don't have one, you can create it as follow:\n",
    "\n",
    "```bash\n",
    "az group create --name huggingface-azure-rg --location eastus\n",
    "```\n",
    "\n",
    "Then, you can ensure that the resource group was created successfully by e.g. listing all the available resource groups that you have access to on your subscription:\n",
    "\n",
    "```bash\n",
    "az group list --output table\n",
    "```\n",
    "\n",
    "More information at [Manage Azure resource groups by using Azure CLI](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/manage-resource-groups-cli).\n",
    "\n",
    "> [!NOTE]\n",
    "> You can also create the Azure Resource Group [via the Azure Portal](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/manage-resource-groups-portal), via the Azure ML Studio when creating the Azure ML Workspace as described below, or [via the Azure Resource Management Python SDK](https://learn.microsoft.com/en-us/azure/developer/python/sdk/examples/azure-sdk-example-resource-group?tabs=bash) (requires it to be installed as `pip install azure-mgmt-resource` in advance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4627c30d-a1a2-4065-8b3d-26560913e90d",
   "metadata": {},
   "source": [
    "### Azure ML Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e70bdc-f12e-4bdf-a818-7fec22681610",
   "metadata": {},
   "source": [
    "An Azure ML workspace under the subscription and resource group aforementioned. If you don't have one, you can create it as:\n",
    "\n",
    "```bash\n",
    "az ml workspace create \\\n",
    "    --name huggingface-azure-ws \\\n",
    "    --resource-group huggingface-azure-rg \\\n",
    "    --location eastus\n",
    "```\n",
    "\n",
    "Then, you can ensure that the workspace was created successfully by e.g. listing all the available workspaces that you have access to on your subscription:\n",
    "\n",
    "```bash\n",
    "az ml workspace list --resource-group huggingface-azure-rg --output table\n",
    "```\n",
    "\n",
    "More information at [Tutorial: Create resources you need to get started - Create the workspace](https://learn.microsoft.com/en-us/azure/machine-learning/concept-workspace?view=azureml-api-2#create-a-workspace) and find more information about Azure ML Workspace at [What is an Azure Machine Learning workspace?](https://learn.microsoft.com/en-us/azure/machine-learning/concept-workspace?view=azureml-api-2).\n",
    "\n",
    "> [!NOTE]\n",
    "> You can also create the Azure ML Workspace [via the Azure ML Studio](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?view=azureml-api-2&tabs=studio#create-a-workspace), [via the Azure Portal](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?view=azureml-api-2&tabs=azure-portal#create-a-workspace), or [via the Azure ML Python SDK](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?view=azureml-api-2&tabs=python#create-a-workspace)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d439f949-f482-4ed9-9d66-0d6ae93d5173",
   "metadata": {},
   "source": [
    "## Setup and installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eade47-ccfa-4add-a10f-933e2190f169",
   "metadata": {},
   "source": [
    "In this example, the [Azure Machine Learning SDK for Python](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/ml/azure-ai-ml) will be used to create the endpoint and the deployment, as well as to invoke the deployed API. Along with it, you will also need to install `azure-identity` to authenticate with your Azure credentials via Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3979198-34e9-48f1-99f1-2c49b447a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-ml azure-identity --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026d4738-831e-4de0-a1df-dcacd05db5b8",
   "metadata": {},
   "source": [
    "More information at [Azure Machine Learning SDK for Python](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-ml-readme?view=azure-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351ec9b-3dbc-4227-82b6-0c71d47a6358",
   "metadata": {},
   "source": [
    "Then, for convenience setting the following environment variables is recommended as those will be used along the example for the Azure ML Client, so make sure to update and set those values accordingly as per your Microsoft Azure account and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5670d3e2-acbb-435b-b851-bca960a2c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env LOCATION eastus\n",
    "%env SUBSCRIPTION_ID <YOUR_SUBSCRIPTION_ID>\n",
    "%env RESOURCE_GROUP <YOUR_RESOURCE_GROUP>\n",
    "%env AML_WORKSPACE_NAME <YOUR_AML_WORKSPACE_NAME>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c2f65-1a8b-419e-b206-5e6f5d9a029d",
   "metadata": {},
   "source": [
    "Finally, you also need to define both the Azure ML Endpoint and Deployment names, as those will be used throughout the example too (note that those need to be unique per region, so add a timestamp or a region-specific identifier if needed; and between 3 and 32 characters long):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e071302-9ea1-43ba-a0a9-8431615f3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env AML_ENDPOINT_NAME qwen-vlm-endpoint\n",
    "%env AML_DEPLOYMENT_NAME qwen-vlm-deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e1c5f-4815-43b7-9227-f9b7c333329d",
   "metadata": {},
   "source": [
    "## Authenticate to Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026488f0-3a44-433e-b84f-76588678ed77",
   "metadata": {},
   "source": [
    "Initially, you need to authenticate to create a new Azure ML client with your credentials, which will be later used to deploy the Hugging Face model, `Qwen/Qwen2.5-VL-32B-Instruct` in this case, into an Azure ML Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe4b31f-34b2-4209-ba99-f3ee99ce2c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "client = MLClient(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    subscription_id=os.getenv(\"SUBSCRIPTION_ID\"),\n",
    "    resource_group_name=os.getenv(\"RESOURCE_GROUP\"),\n",
    "    workspace_name=os.getenv(\"AML_WORKSPACE_NAME\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2498c42-2e9a-4d0f-9c51-2d9775a92eaf",
   "metadata": {},
   "source": [
    "### Create and Deploy Managed Online Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0adfbfd-ca52-4d8c-992b-8d2b72b497d3",
   "metadata": {},
   "source": [
    "Before creating the Azure ML Endpoint, you need to build the Azure ML Model URI, which is formatted as it follows `azureml://registries/<REGISTRY_NAME>/models/<MODEL_ID>/labels/latest`, that means that the `REGISTRY_NAME` should be set to \"HuggingFace\" as you intend to deploy a model from the Hugging Face Collection on the Azure ML Model Catalog, and the `MODEL_ID` won't be the Hugging Face Hub ID, but rather the ID with hyphen replacements for both backslash (/) and underscores (_), as it follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a77a0-3ce3-418d-b161-757a2b09232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2.5-VL-32B-Instruct\"\n",
    "\n",
    "uri = f\"azureml://registries/HuggingFace/models/{model_id.replace('/', '-').replace('_', '-')}/labels/latest\"\n",
    "uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a432367e-e39a-409e-845d-9ea38406359d",
   "metadata": {},
   "source": [
    "Note that you will need to verify in advance that the URI is valid, and that the given Hugging Face Hub Model ID exists on Azure, since Hugging Face is publishing those models into their collection, meaning that some models may be available on the Hugging Face Hub but not yet on the Azure ML Model Catalog (you can request adding a model following the guide [Request a model addition](https://huggingface.co/docs/microsoft-azure/guides/request-model-addition)).\n",
    "\n",
    "Alternatively, you can use the following snippet to verify if a model is available on the Azure ML Model Catalog programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01768a4-c860-497e-bdea-1e9528b09469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(f\"https://generate-azureml-urls.azurewebsites.net/api/generate?modelId={model_id}\")\n",
    "if response.status_code != 200:\n",
    "    print(\"[{response.status_code=}] {model_id=} not available on the Hugging Face Collection in Azure ML Model Catalog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287f007e-b62e-449b-818c-cb35a178aa50",
   "metadata": {},
   "source": [
    "Then, once the model URI has been built correctly and that the model exists on Azure ML, then you can create the Managed Online Endpoint specifying its name (note that the name must be unique per region, so it's a nice practice to add some sort of unique name to it in case multi-region deployments are intended) via the [ManagedOnlineEndpoint Python class](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.managedonlineendpoint?view=azure-python).\n",
    "\n",
    "Also note that by default the `ManagedOnlineEndpoint` will use the `key` authentication method, meaning that there will be a primary and secondary key that should be sent within the Authentication headers as a Bearer token; but also the `aml_token` authentication method can be used, read more about it at [Authenticate clients for online endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-authenticate-online-endpoint).\n",
    "\n",
    "The deployment, created via the [ManagedOnlineDeployment Python class](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.managedonlinedeployment?view=azure-python), will define the actual model deployment that will be exposed via the previously created endpoint. The `ManagedOnlineDeployment` will expect: the `model` i.e., the previously created URI `azureml://registries/HuggingFace/models/Qwen-Qwen2.5-VL-32B-Instruct/labels/latest`, the `endpoint_name`, and the instance requirements being the `instance_type` and the `instance_count`.\n",
    "\n",
    "Every model in the Hugging Face Collection is powered by an efficient inference backend, and each of those can run on a wide variety of instance types (as listed in [Supported Hardware](https://huggingface.co/docs/supported-hardware)); in this case, a NVIDIA H100 GPU will be used i.e., `Standard_NC40ads_H100_v5`.\n",
    "\n",
    "> [!WARNING]\n",
    "> Since for some models and inference engines you need to run those on a GPU-accelerated instance, you may need to request a quota increase for some of the supported instances as per the model you want to deploy. Also, keep into consideration that each model comes with a list of all the supported instances, being the recommended one for each tier the lower instance in terms of available VRAM. Read more about quota increase requests for Azure ML at [Manage and increase quotas and limits for resources with Azure Machine Learning](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-quotas?view=azureml-api-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e8cd42-5ea4-44a7-b998-dbd1cbf9c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment\n",
    "\n",
    "endpoint = ManagedOnlineEndpoint(name=os.getenv(\"AML_ENDPOINT_NAME\"))\n",
    "\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    name=os.getenv(\"AML_DEPLOYMENT_NAME\"),\n",
    "    endpoint_name=os.getenv(\"AML_ENDPOINT_NAME\"),\n",
    "    model=\"azureml://registries/HuggingFace/models/qwen-qwen2.5-vl-32b-instruct/labels/latest\",\n",
    "    instance_type=\"Standard_NC40ads_H100_v5\",\n",
    "    instance_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d81645-0a8b-4388-b250-13564ea86b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.begin_create_or_update(endpoint).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646eb1ae-1497-4303-9631-e5bab3022301",
   "metadata": {},
   "source": [
    "![Azure ML Endpoint from Azure ML Studio](./azure-ml-endpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e82cd-3bd4-4ecf-a5bf-6423b37733ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.online_deployments.begin_create_or_update(deployment).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b135caa-a700-4e5c-8f2b-48b9a709c8b2",
   "metadata": {},
   "source": [
    "![Azure ML Deployment from Azure ML Studio](./azure-ml-deployment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54884da-0413-438c-8f76-b22302149e4d",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> Note that whilst the Azure ML Endpoint creation is relatively fast, the deployment will take longer since it needs to allocate the resources on Azure so expect it to take ~10-15 minutes, but it could aswell take longer depending on the instance provisioning and availability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4cbaca-2fdb-4924-89bf-b3a6c54c998b",
   "metadata": {},
   "source": [
    "Once deployed, via the Azure ML Studio you'll be able to inspect the logs at https://ml.azure.com/endpoints/realtime/qwen-vlm-endpoint/logs, see how to consume the deployed API at https://ml.azure.com/endpoints/realtime/qwen-vlm-endpoint/consume, or check their [(on preview) model monitoring feature](https://learn.microsoft.com/en-us/azure/machine-learning/concept-model-monitoring?view=azureml-api-2) at https://ml.azure.com/endpoints/realtime/qwen-vlm-endpoint/Monitoring.\n",
    "\n",
    "> [!NOTE]\n",
    "> If you named your Azure ML Endpoint differently (set via the `AML_ENDPOINT_NAME` environment variable, you'll need to update the URLs above as https://ml.azure.com/endpoints/realtime/<AML_ENDPOINT_NAME> for those to work as expected.\n",
    "\n",
    "More information about the [Azure ML Managed Online Endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints-online?view=azureml-api-2#managed-online-endpoints) and [Deploy and score a machine learning model by using an online endpoint](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-online-endpoints) (which can be deployed via the `az` CLI, the Azure ML SDK for Python as above, from the Azure ML Studio, from the Hugging Face Hub from the given model card, or from an ARM Template)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614189ee-a224-404a-bd5a-2347d7404c5f",
   "metadata": {},
   "source": [
    "## Send requests to the Azure ML Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9b306-7331-4fc3-bb65-d7e2134e7d5e",
   "metadata": {},
   "source": [
    "Finally, now that the Azure ML Endpoint is deployed, you can send requests to it. In this case, since the task of the model is `text-generation` (also known as `chat-completion`) you can either use the default scoring endpoint, being `/generate` which is the standard text generation endpoint without chat capabilities (as leveraging the chat template or having an OpenAI-compatible OpenAPI interface), or alternatively just benefit from the fact that Text Generation Inference (TGI) i.e., the inference engine in which the model is running on top, exposes OpenAI-compatible routes.\n",
    "\n",
    "> [!NOTE]\n",
    "> Note that below only some of the options are listed, but you can send requests to the deployed endpoint as long as you send the HTTP requests with the `azureml-model-deployment` header set to the name of the Azure ML Deployment (not the Endpoint), and have the necessary authentication token / key to send requests to the given endpoint; then you can send HTTP request to all the routes that the backend engine is exposing, not only to the scoring route."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8fc287-e86c-4a16-b9fa-82f8db2509fa",
   "metadata": {},
   "source": [
    "### Azure Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de6826-90fd-4ccf-9c95-cf7197d34afb",
   "metadata": {},
   "source": [
    "You can invoke the Azure ML Endpoint on the scoring route, in this case `/generate` (more information about it in the [Qwen2.5-VL-32B-Instruct page on AzureML](https://ml.azure.com/models/qwen-qwen2.5-vl-32b-instruct/version/1/catalog/registry/HuggingFace)), via the Azure Python SDK with the previously instantiated `azure.ai.ml.MLClient` (or instantiate a new one if working from a different session) as it follows:\n",
    "\n",
    "> [!NOTE]\n",
    "> Since in this case you are deploying a Vision Language Model (VLM), to leverage the vision capabilities through the `/generate` endpoint you will need to include either the image URL or the base64 encoding of the image formatted in Markdown as e.g. `![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rabbit.png)What is this a picture of?\\n\\n` or `![](data:image/png;base64,...)What is this a picture of?\\n\\n`.\n",
    "> More information at [Vision Language Model Inference in TGI](https://huggingface.co/docs/text-generation-inference/basic_tutorials/visual_language_models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99c103b-f39e-433d-af15-5cb9776458b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(mode=\"w+\", delete=True, suffix=\".json\") as tmp:\n",
    "    json.dump({\n",
    "        \"inputs\": \"![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rabbit.png)What is this a picture of?\\n\\n\",\n",
    "        \"parameters\": {\"max_new_tokens\": 128}\n",
    "    }, tmp)\n",
    "    \n",
    "    tmp.flush()\n",
    "\n",
    "    response = client.online_endpoints.invoke(\n",
    "        endpoint_name=os.getenv(\"AML_ENDPOINT_NAME\"),\n",
    "        deployment_name=os.getenv(\"AML_DEPLOYMENT_NAME\"),\n",
    "        request_file=tmp.name,\n",
    "    )\n",
    "\n",
    "print(json.loads(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e4439-b590-4f3b-ad84-62c2751ed42b",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> Note that the Azure ML Python SDK requires a path to a JSON file when invoking the endpoints, meaning that whatever payload you want to send to the endpoint will need to be first converted into a JSON file, whilst that only applies to the requests sent via the Azure ML Python SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d6e4b5-9f99-49cd-9b2a-09d8bb1e69bd",
   "metadata": {},
   "source": [
    "### OpenAI Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d470af7-0fee-4e20-97a4-e64a7e8f4071",
   "metadata": {},
   "source": [
    "Since Text Generation Inference (TGI) also exposes OpenAI-compatible routes, you can also leverage the OpenAI Python SDK to send requests to the deployed Azure ML Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37703050-df09-4825-9328-b591ef53632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c2bfad-a1e8-483f-888b-2793097fb01a",
   "metadata": {},
   "source": [
    "To use the OpenAI Python SDK with Azure ML, you need to first retrieve both the `api_url` with the `/v1` route (that contains the `v1/chat/completions` endpoint that the OpenAI Python SDK will send requests to), and the `api_key` which is the primary key generated in Azure ML (unless a dedicated Azure ML Token is used instead), which you can do via the previously instantiated `azure.ai.ml.MLClient` as it follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dddbe29-8498-46d5-bc71-6903cf8a36ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = client.online_endpoints.get_keys(os.getenv(\"AML_ENDPOINT_NAME\")).primary_key\n",
    "api_url = client.online_endpoints.get(os.getenv(\"AML_ENDPOINT_NAME\")).scoring_uri.replace(\"/generate\", \"/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29679b34-3982-4ed6-92a5-6136d596310d",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> Alternatively, you can also build the API URL manually as it follows:\n",
    "> ```python\n",
    "> api_url = f\"https://{os.getenv('AML_ENDPOINT_NAME')}.{os.getenv('LOCATION')}.inference.ml.azure.com/v1\"\n",
    "> api_url\n",
    "> ```\n",
    "> Or just retrieve it from the Azure ML Studio manually too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1e2ff6-c135-43c2-b466-49c9054030ef",
   "metadata": {},
   "source": [
    "Then you can use the OpenAI Python SDK normally, making sure to include the extra headers required by Azure ML, being the `azureml-model-deployment` header that contains the Azure ML Deployment name, that can either be set within each call to `chat.completions.create` via the `extra_headers` parameter as commented below, or also via the `default_headers` parameter when instantiating the `OpenAI` client (which is the recommended approach since the header needs to always be present, so setting it once is preferred)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa18d5-04f5-4876-820b-d416e91e588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI(\n",
    "    base_url=api_url,\n",
    "    api_key=api_key,\n",
    "    default_headers={\"azureml-model-deployment\": os.getenv(\"AML_DEPLOYMENT_NAME\")},\n",
    ")\n",
    "\n",
    "completion = openai_client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-VL-32B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that responds like a pirate.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What is in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rabbit.png\"\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=128,\n",
    "    # extra_headers={\"azureml-model-deployment\": os.getenv(\"AML_DEPLOYMENT_NAME\")},\n",
    ")\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a2463a-b968-4f35-8553-98c3e507123d",
   "metadata": {},
   "source": [
    "### cURL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f92a80-0001-445f-9e07-c00643721a79",
   "metadata": {},
   "source": [
    "Alternatively, you can also just use `cURL` to send requests to the deployed endpoint, with the `api_url` and `api_key` values programmatically retrieved in the OpenAI snippet and now set as environment variables so that `cURL` can use those, as it follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccea9fb-3749-4e9b-ac3b-51304d57fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"API_URL\"] = api_url\n",
    "os.environ[\"API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d88e895-61fc-4b56-831a-76c8ca9ece39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -sS $API_URL/chat/completions \\\n",
    "    -H \"Authorization: Bearer $API_KEY\" \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -H \"azureml-model-deployment: $AML_DEPLOYMENT_NAME\" \\\n",
    "    -d '{ \\\n",
    "\"messages\":[ \\\n",
    "    {\"role\":\"system\",\"content\":\"You are an assistant that replies like a pirate.\"}, \\\n",
    "    {\"role\":\"user\",\"content\": [ \\\n",
    "        {\"type\":\"text\",\"text\":\"What is in this image?\"}, \\\n",
    "        {\"type\":\"image_url\",\"image_url\":{\"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rabbit.png\"}} \\\n",
    "    ]} \\\n",
    "], \\\n",
    "\"max_tokens\":128 \\\n",
    "}' | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464c9356-be10-4ec8-aa48-195245c2de82",
   "metadata": {},
   "source": [
    "You can also just go to the Azure ML endpoint in the Azure ML Studio and retrieve both the URL (note that it will default to the `/generate` endpoint, but to use the OpenAI-compatible layer you need to use the `/v1/chat/completions` endpoint instead) and the API Key values, as well as the Azure ML Model Deployment name for the given model, and then send the request as follows after replacing the values from Azure ML:\n",
    "\n",
    "```bash\n",
    "curl -sS <API_URL>/v1/chat/completions \\\n",
    "    -H \"Authorization: Bearer <PRIMARY_KEY>\" \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -H \"azureml-model-deployment: $AML_DEPLOYMENT_NAME\" \\\n",
    "    -d '{ \\\n",
    "\"messages\":[ \\\n",
    "    {\"role\":\"system\",\"content\":\"You are an assistant that replies like a pirate.\"}, \\\n",
    "    {\"role\":\"user\",\"content\": [ \\\n",
    "        {\"type\":\"text\",\"text\":\"What is in this image?\"}, \\\n",
    "        {\"type\":\"image_url\",\"image_url\":{\"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rabbit.png\"}} \\\n",
    "    ]} \\\n",
    "], \\\n",
    "\"max_tokens\":128 \\\n",
    "}' | jq\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5940423f-22fb-4c5a-98ef-852634d1e2bf",
   "metadata": {},
   "source": [
    "### Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0c194-dd5b-4ea6-b984-566aea47f8d2",
   "metadata": {},
   "source": [
    "[Gradio](https://www.gradio.app/) is the fastest way to demo your machine learning model with a friendly web interface so that anyone can use it. You can also leverage the OpenAI Python SDK to build a simple multimodal (text and images) `ChatInterface` that you can use within the Jupyter Notebook cell where you are running it.\n",
    "\n",
    "> [!NOTE]\n",
    "> Ideally you could deploy the Gradio Chat Interface connected to your Azure ML Managed Online Endpoint as an Azure Container App as described in [Tutorial: Build and deploy from source code to Azure Container Apps](https://learn.microsoft.com/en-us/azure/container-apps/tutorial-deploy-from-code?tabs=python). If you'd like us to show you how to do it for Gradio in particular, feel free to [open an issue requesting it](https://github.com/huggingface/Microsoft-Azure/issues/new)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5ae688-cdb5-4751-aa8f-ca2178668185",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gradio --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849062e1-fc55-4e1e-8c0b-88daea23e76f",
   "metadata": {},
   "source": [
    "See below an example on how to leverage Gradio's `ChatInterface`, or find more information about it at [Gradio ChatInterface Docs](https://www.gradio.app/docs/gradio/chatinterface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377ad7c-ecd7-4e98-9910-b4afc5dc393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "from typing import Dict, Iterator, List, Literal\n",
    "\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI(\n",
    "    base_url=os.getenv(\"API_URL\"),\n",
    "    api_key=os.getenv(\"API_KEY\"),\n",
    "    default_headers={\"azureml-model-deployment\": os.getenv(\"AML_DEPLOYMENT_NAME\")}\n",
    ")\n",
    "\n",
    "def predict(\n",
    "    message: Dict[str, str | List[str]],\n",
    "    history: List[Dict[Literal[\"role\", \"content\"], str]]\n",
    ") -> Iterator[str]:\n",
    "    content = []\n",
    "    if message[\"text\"]:\n",
    "        content.append({\"type\": \"text\", \"text\": message[\"text\"]})\n",
    "    \n",
    "    for file_path in message.get(\"files\", []):\n",
    "        with open(file_path, \"rb\") as image_file:\n",
    "            base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "            content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"},\n",
    "            })\n",
    "    \n",
    "    messages = history.copy()\n",
    "    messages.append({\"role\": \"user\", \"content\": content})\n",
    "\n",
    "    stream = openai_client.chat.completions.create(\n",
    "        model=\"Qwen/Qwen2.5-VL-32B-Instruct\",\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "    )\n",
    "    buffer = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            buffer += chunk.choices[0].delta.content\n",
    "            yield buffer\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    predict,\n",
    "    textbox=gr.MultimodalTextbox(\n",
    "        label=\"Input\",\n",
    "        file_types=[\".jpg\", \".png\", \".jpeg\"],\n",
    "        file_count=\"multiple\"\n",
    "    ),\n",
    "    multimodal=True,\n",
    "    type=\"messages\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b5549e-345f-4b21-88c2-e1019a3f30b4",
   "metadata": {},
   "source": [
    "![Gradio Chat Interface with Azure ML Endpoint](./azure-ml-gradio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf20842-e691-4f8c-949c-fb3fc1038336",
   "metadata": {},
   "source": [
    "## Release resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c475ca0a-cea2-45bc-b93c-3824fc73cf6d",
   "metadata": {},
   "source": [
    "Once you are done using the Azure ML Endpoint / Deployment, you can delete the resources as it follows, meaning that you will stop paying for the instance on which the model is running and all the attached costs will be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd52337-cce6-431c-b036-fb91d0a894ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.online_endpoints.begin_delete(name=os.getenv(\"AML_ENDPOINT_NAME\")).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa2816f-bae1-48e4-8b4d-753b34a149ac",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d01c0b-44fc-40b6-bfc7-7ba8dcb1432f",
   "metadata": {},
   "source": [
    "Throughout this example you learnt how to create and configure your Azure account for Azure ML, how to then create an Azure ML Managed Online Endpoint running a model from the Hugging Face Collection in the Azure ML Model Catalog, how to send inference requests to it afterwards with different approaches, as well as how to stop the resources.\n",
    "\n",
    "If you have any doubt, issue or question about this example, feel free to [open an issue](https://github.com/huggingface/Microsoft-Azure/issues/new) and we'll do our best to help!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
