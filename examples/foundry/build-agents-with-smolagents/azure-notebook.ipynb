{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa3b3fb3-ccce-4654-8a3a-02bb4016eeaf",
   "metadata": {},
   "source": [
    "# Build Agents with smolagents on Microsoft Foundry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975fe589-9800-4124-a3be-d9f618da61a4",
   "metadata": {},
   "source": [
    "This example showcases how to build agents with [`smolagents`](https://github.com/huggingface/smolagents), leveraging Large Language Models (LLMs) from the Hugging Face collection on Microsoft Foundry (formerly Azure AI Foundry) deployed as an Azure Machine Learning Managed Online Endpoint.\n",
    "\n",
    "> [!WARNING]\n",
    "> This example is not intended to be a in-detail example on how to deploy Large Language Models (LLMs) on Microsoft Foundry but rather focused on how to build agents with it, this being said, it's highly recommended to read more about Microsoft Foundry deployments in the example [\"Deploy Large Language Models (LLMs) on Microsoft Foundry\"](https://huggingface.co/docs/microsoft-azure/azure-ai/examples/deploy-large-language-models).\n",
    "\n",
    "TL;DR Smolagents is an open-source Python library designed to make it extremely easy to build and run agents using just a few lines of code. Microsoft Foundry (formerly Azure AI Foundry) provides a unified platform for enterprise AI operations, model builders, and application development. Azure Machine Learning is a cloud service for accelerating and managing the machine learning (ML) project lifecycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d723dca0-9016-4b7c-a21b-ff01f75ea440",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef145992-e426-491d-b218-87ecd9d06d65",
   "metadata": {},
   "source": [
    "This example shows how to deploy [`Qwen/Qwen2.5-Coder-32B-Instruct`](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct) from the Hugging Face Hub (or see it on [Azure Machine Learning](https://ml.azure.com/models/qwen-qwen2.5-coder-32b-instruct/version/2/catalog/registry/HuggingFace) or on [Microsoft Foundry](https://ai.azure.com/explore/models/qwen-qwen2.5-coder-32b-instruct/version/2/registry/HuggingFace)) as an Azure Machine Learning Managed Online Endpoint on Microsoft Foundry.\n",
    "\n",
    "Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen), bringing the following improvements upon CodeQwen1.5:\n",
    "\n",
    "- Significantly improvements in code generation, code reasoning and code fixing. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o.\n",
    "- A more comprehensive foundation for real-world applications such as Code Agents. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\n",
    "- Long-context Support up to 128K tokens.\n",
    "\n",
    "![Qwen2.5 Coder 32B Instruct on the Hugging Face Hub](./qwen2.5-coder-hub.png)\n",
    "\n",
    "![Qwen2.5 Coder 32B Instruct on Azure AI Foundry](./qwen2.5-coder-azure-ai.png)\n",
    "\n",
    "For more information, make sure to check [their model card on the Hugging Face Hub](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct/blob/main/README.md).\n",
    "\n",
    "> [!NOTE]\n",
    "> Note that you can select any LLM available on the Hugging Face Hub with the \"Deploy on Microsoft Foundry\" option enabled, or directly select any of the LLMs available in either the Azure Machine Learning or Microsoft Foundry model catalog under the \"HuggingFace\" collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763a3a3a-aae1-43be-a925-0a4e1f5493a3",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61e81c6-3b27-48ac-b74d-d9bfa5850679",
   "metadata": {},
   "source": [
    "To run the following example, you will need to comply with the following pre-requisites, alternatively, you can also read more about those in the [Azure Machine Learning Tutorial: Create resources you need to get started](https://learn.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources?view=azureml-api-2).\n",
    "\n",
    "- An Azure account with an active subscription.\n",
    "- The Azure CLI installed and logged in.\n",
    "- The Azure Machine Learning extension for the Azure CLI.\n",
    "- An Azure Resource Group.\n",
    "- A Hub-based project on Microsoft Foundry.\n",
    "\n",
    "For more information, please go through the steps in the guide [\"Configure Azure Machine Learning and Microsoft Foundry\"](https://huggingface.co/docs/microsoft-azure/guides/configure-azure-ml-microsoft-foundry)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d439f949-f482-4ed9-9d66-0d6ae93d5173",
   "metadata": {},
   "source": [
    "## Setup and installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eade47-ccfa-4add-a10f-933e2190f169",
   "metadata": {},
   "source": [
    "In this example, the [Azure Machine Learning SDK for Python](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/ml/azure-ai-ml) will be used to create the endpoint and the deployment, as well as to invoke the deployed API. Along with it, you will also need to install `azure-identity` to authenticate with your Azure credentials via Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3979198-34e9-48f1-99f1-2c49b447a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-ml azure-identity --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026d4738-831e-4de0-a1df-dcacd05db5b8",
   "metadata": {},
   "source": [
    "More information at [Azure Machine Learning SDK for Python](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-ml-readme?view=azure-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351ec9b-3dbc-4227-82b6-0c71d47a6358",
   "metadata": {},
   "source": [
    "Then, for convenience setting the following environment variables is recommended as those will be used along the example for the Azure Machine Learning Client, so make sure to update and set those values accordingly as per your Microsoft Azure account and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5670d3e2-acbb-435b-b851-bca960a2c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env LOCATION eastus\n",
    "%env SUBSCRIPTION_ID <YOUR_SUBSCRIPTION_ID>\n",
    "%env RESOURCE_GROUP <YOUR_RESOURCE_GROUP>\n",
    "%env WORKSPACE_NAME <YOUR_WORKSPACE_NAME>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680796a3-c95c-4dce-8e84-cbafafc9057d",
   "metadata": {},
   "source": [
    "Finally, you also need to define both the endpoint and deployment names, as those will be used throughout the example too:\n",
    "\n",
    "> [!NOTE]\n",
    "> Note that endpoint names must to be globally unique per region i.e., even if you don't have any endpoint named that way running under your subscription, if the name is reserved by another Azure customer, then you won't be able to use the same name. Adding a timestamp or a custom identifier is recommended to prevent running into HTTP 400 validation issues when trying to deploy an endpoint with an already locked / reserved name. Also the endpoint name must be between 3 and 32 characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714cd833-1e1d-4ae9-9a86-6807706be987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from uuid import uuid4\n",
    "\n",
    "os.environ[\"ENDPOINT_NAME\"] = f\"qwen-coder-endpoint-{str(uuid4())[:8]}\"\n",
    "os.environ[\"DEPLOYMENT_NAME\"] = f\"qwen-coder-deployment-{str(uuid4())[:8]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e1c5f-4815-43b7-9227-f9b7c333329d",
   "metadata": {},
   "source": [
    "## Authenticate to Azure Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026488f0-3a44-433e-b84f-76588678ed77",
   "metadata": {},
   "source": [
    "Initially, you need to authenticate into Microsoft Foundry Hub via Azure Machine Learning with the Azure Machine Learning Python SDK, which will be later used to deploy `Qwen/Qwen2.5-Coder-32B-Instruct` as an Azure Machine Learning Managed Online Endpoint on Microsoft Foundry.\n",
    "\n",
    "> [!NOTE]\n",
    "> On standard Azure Machine Learning deployments you'd need to create the `MLClient` using the Azure Machine Learning Workspace as the `workspace_name` whereas for Microsoft Foundry, you need to provide Microsoft Foundry Hub name as the `workspace_name` instead, and that will deploy the endpoint under Microsoft Foundry too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe4b31f-34b2-4209-ba99-f3ee99ce2c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "client = MLClient(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    subscription_id=os.getenv(\"SUBSCRIPTION_ID\"),\n",
    "    resource_group_name=os.getenv(\"RESOURCE_GROUP\"),\n",
    "    workspace_name=os.getenv(\"WORKSPACE_NAME\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2498c42-2e9a-4d0f-9c51-2d9775a92eaf",
   "metadata": {},
   "source": [
    "## Create and Deploy Foundry Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a72413-08d3-479a-8e44-ce4dbc8e9916",
   "metadata": {},
   "source": [
    "Before creating the Managed Online Endpoint, you need to build the model URI, which is formatted as it follows `azureml://registries/HuggingFace/models/<MODEL_ID>/labels/latest` where the `MODEL_ID` won't be the Hugging Face Hub ID but rather its name on Azure, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a77a0-3ce3-418d-b161-757a2b09232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
    "\n",
    "model_uri = f\"azureml://registries/HuggingFace/models/{model_id.replace('/', '-').replace('_', '-').lower()}/labels/latest\"\n",
    "model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a432367e-e39a-409e-845d-9ea38406359d",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> To check if a model from the Hugging Face Hub is available in Azure, you should read about it in [Supported Models](https://huggingface.co/docs/microsoft-azure/azure-ai/models). If not, you can always [Request a model addition in the Hugging Face collection on Azure](https://huggingface.co/docs/microsoft-azure/guides/request-model-addition))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754a350-9a2b-43ee-90b5-615b1ad74153",
   "metadata": {},
   "source": [
    "Then you need to create the [ManagedOnlineEndpoint via the Azure Machine Learning Python SDK](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.managedonlineendpoint?view=azure-python) as follows.\n",
    "\n",
    "> [!NOTE]\n",
    "> Every model in the Hugging Face collection is powered by an efficient inference backend, and each of those can run on a wide variety of instance types (as listed in [Supported Hardware](https://huggingface.co/docs/microsoft-azure/azure-ai/supported-hardware)). Since for models and inference engines require a GPU-accelerated instance, you might need to request a quota increase as per [Manage and increase quotas and limits for resources with Azure Machine Learning](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-quotas?view=azureml-api-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e8cd42-5ea4-44a7-b998-dbd1cbf9c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment\n",
    "\n",
    "endpoint = ManagedOnlineEndpoint(name=os.getenv(\"ENDPOINT_NAME\"))\n",
    "\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    name=os.getenv(\"DEPLOYMENT_NAME\"),\n",
    "    endpoint_name=os.getenv(\"ENDPOINT_NAME\"),\n",
    "    model=model_uri,\n",
    "    instance_type=\"Standard_NC40ads_H100_v5\",\n",
    "    instance_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d81645-0a8b-4388-b250-13564ea86b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.begin_create_or_update(endpoint).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646eb1ae-1497-4303-9631-e5bab3022301",
   "metadata": {},
   "source": [
    "![Azure AI Endpoint from Azure AI Foundry](./azure-ai-endpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae01066-228b-45c3-95b4-30fc7661b0f9",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> On Microsoft Foundry the endpoint will only be listed within the \"My assets -> Models + endpoints\" tab once the deployment is created, not before as in Azure Machine Learning where the endpoint is shown even if it doesn't contain any active or in-progress deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e82cd-3bd4-4ecf-a5bf-6423b37733ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.online_deployments.begin_create_or_update(deployment).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b135caa-a700-4e5c-8f2b-48b9a709c8b2",
   "metadata": {},
   "source": [
    "![Azure AI Deployment from Azure AI Foundry](./azure-ai-deployment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f112d21e-53c2-4340-bcc0-28499b616ae5",
   "metadata": {},
   "source": [
    "The deployment might take ~10-15 minutes, but it could as well take longer depending on the selected SKU availability in the region. Once deployed, you will be able to inspect the endpoint details, the real-time logs, how to consume the endpoint, and [monitoring (on preview)](https://learn.microsoft.com/en-us/azure/machine-learning/concept-model-monitoring?view=azureml-api-2).\n",
    "\n",
    "Find more information about it at [Azure Machine Learning Managed Online Endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints-online?view=azureml-api-2#managed-online-endpoints)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614189ee-a224-404a-bd5a-2347d7404c5f",
   "metadata": {},
   "source": [
    "## Build agents with smolagents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3ef603-f5dd-4145-8220-6874e443f03e",
   "metadata": {},
   "source": [
    "Now that the Azure AI Endpoint is running, you can start sending requests to it. Since there are multiple approaches, but the following is just covering the OpenAI Python SDK approach, you should visit e.g. [Deploy Large Language Models (LLMs) on Azure AI](https://huggingface.co/docs/microsoft-azure/azure-ai/examples/deploy-large-language-models) to see different alternatives.\n",
    "\n",
    "So on, the steps to follow for building the agent are going to be:\n",
    "\n",
    "1. Create the OpenAI client with `smolagents`, connected to the running Azure AI Endpoint via the `smolagents.OpenAIServerModel` (note that `smolagents` also exposes the `smolagents.AzureOpenAIServerModel` but that's the client for using OpenAI via the Azure, not to connect to Azure AI).\n",
    "2. Define the set of tools that the agent will have access to i.e., Python functions with the `smolagents.tool` decorator.\n",
    "3. Create the `smolagents.CodeAgent` leveraging the code-LLM deployed on Azure AI, adding the set tools previously defined, so that the agent can use those when appropriate, using a local executor (not recommended if code to be executed is sensible or unidentified)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec38ed2-3caf-441f-b1f3-d92292bec573",
   "metadata": {},
   "source": [
    "### Create OpenAI Client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5fd4c3d3-f8f9-4e3d-80ad-7d9460b8e5f0",
   "metadata": {},
   "source": [
    "Since every LLM in the Hugging Face catalog is deployed with an inference engine that exposes OpenAI-compatible routes, you can also leverage the OpenAI Python SDK via `smolagents` to send requests to the deployed Azure Machine Learning Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e02e8-4c2e-46f3-bffa-82f9e23f1d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"smolagents[openai]\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67d140d-da47-447c-9689-0a3cfbb0d914",
   "metadata": {},
   "source": [
    "To use the OpenAI Python SDK with Azure Machine Learning Managed Online Endpoints, you need to first retrieve:\n",
    "\n",
    "- `api_url` with the `/v1` route (that contains the `v1/chat/completions` endpoint that the OpenAI Python SDK will send requests to)\n",
    "- `api_key` which is the API Key in Azure AI or the primary key in Azure Machine Learning (unless a dedicated Azure Machine Learning Token is used instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c0235-6e9e-4f12-ba35-eac258356a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlsplit\n",
    "\n",
    "api_key = client.online_endpoints.get_keys(os.getenv(\"ENDPOINT_NAME\")).primary_key\n",
    "\n",
    "url_parts = urlsplit(client.online_endpoints.get(os.getenv(\"ENDPOINT_NAME\")).scoring_uri)\n",
    "api_url = f\"{url_parts.scheme}://{url_parts.netloc}/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc5dba4-ced4-4e5f-ae6c-cf5236730b7b",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> Alternatively, you can also build the API URL manually as it follows, since the URIs are globally unique per region, meaning that there will only be one endpoint named the same way within the same region:\n",
    "> ```python\n",
    "> api_url = f\"https://{os.getenv('ENDPOINT_NAME')}.{os.getenv('LOCATION')}.inference.ml.azure.com/v1\"\n",
    "> ```\n",
    "> Or just retrieve it from either Microsoft Foundry or the Azure Machine Learning Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33cf713-7312-4021-97a6-19e6ae481a03",
   "metadata": {},
   "source": [
    "Then you can use the OpenAI Python SDK normally, making sure to include the extra header `azureml-model-deployment` header that contains the Azure AI / ML Deployment name.\n",
    "\n",
    "The extra header will be provided via the `default_headers` argument of the OpenAI Python SDK when instantiating the client, to be provided in `smolagents` via the `client_kwargs` argument of `smolagents.OpenAIServerModel`, that will propagate those to the underlying `OpenAI` client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cb767c-3358-4ea2-b7db-9613c34d4b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import OpenAIServerModel\n",
    "\n",
    "model = OpenAIServerModel(\n",
    "    model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    api_base=api_url,\n",
    "    api_key=api_key,\n",
    "    client_kwargs={\"default_headers\": {\"azureml-model-deployment\": os.getenv(\"DEPLOYMENT_NAME\")}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828105d3-e9a3-4b1b-b017-4e6daa45e61b",
   "metadata": {},
   "source": [
    "### Build Python Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df23e70-f711-4a3b-b775-e111464de31d",
   "metadata": {},
   "source": [
    "`smolagents` will be used to build the tools that the agent will leverage, as well as to build the `smolagents.CodeAgent` itself. The following tools will be defined, using the `smolagents.tool` decorator, that will prepare the Python functions to be used as tools within the LLM Agent.\n",
    "\n",
    "Note that the function signatures should come with proper typing so as to guide the LLM, as well as a clear function name and, most importantly, well-formatted docstrings indicating what the function does, what are the arguments, what it returns, and what errors can be raised; if applicable.\n",
    "\n",
    "In this case, the tools that will be provided to the agent are the following:\n",
    "\n",
    "- World Time API - `get_time_in_timezone`: fetches the current time on a given location using the World Time API.\n",
    "\n",
    "- Wikipedia API - `search_wikipedia`: fetches a summary of a Wikipedia entry using the Wikipedia API.\n",
    "\n",
    "> [!NOTE]\n",
    "> In this case for the sake of simplicity, the tools to be used have been ported from https://github.com/huggingface/smolagents/blob/main/examples/multiple_tools.py, so all the credit goes to the original authors and maintainers of the `smolagents` GitHub repository. Also only the tools for querying the World Time API and the Wikipedia API have been kept, since those have a generous Free Tier that allows anyone to use those without paying or having to create an account / API token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07ab194-04d9-4de8-aebf-35324f6ec329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75724d2-dd3b-40d9-bc98-fd7e8e528fd7",
   "metadata": {},
   "source": [
    "#### World Time API - `get_time_in_timezone`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ffc84-614a-4378-90a8-752cce3906c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_time_in_timezone(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the current time for a given location using the World Time API.\n",
    "    Args:\n",
    "        location: The location for which to fetch the current time, formatted as 'Region/City'.\n",
    "    Returns:\n",
    "        str: A string indicating the current time in the specified location, or an error message if the request fails.\n",
    "    Raises:\n",
    "        requests.exceptions.RequestException: If there is an issue with the HTTP request.\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    \n",
    "    url = f\"http://worldtimeapi.org/api/timezone/{location}.json\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "        current_time = data[\"datetime\"]\n",
    "\n",
    "        return f\"The current time in {location} is {current_time}.\"\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error fetching time data: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b31ad64-0a6a-49de-815f-1510a51170c4",
   "metadata": {},
   "source": [
    "#### Wikipedia API - `search_wikipedia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936a0ed4-2ccd-47b9-a8ae-430f1b0d2509",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches a summary of a Wikipedia page for a given query.\n",
    "    Args:\n",
    "        query: The search term to look up on Wikipedia.\n",
    "    Returns:\n",
    "        str: A summary of the Wikipedia page if successful, or an error message if the request fails.\n",
    "    Raises:\n",
    "        requests.exceptions.RequestException: If there is an issue with the HTTP request.\n",
    "    \"\"\"\n",
    "    import requests\n",
    "\n",
    "    url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{query}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "        title = data[\"title\"]\n",
    "        extract = data[\"extract\"]\n",
    "\n",
    "        return f\"Summary for {title}: {extract}\"\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error fetching Wikipedia data: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859542cd-6008-4acd-8c07-a6d6825f68be",
   "metadata": {},
   "source": [
    "### Create Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ae0ab3-be45-4315-8ad0-286d5d95b7e4",
   "metadata": {},
   "source": [
    "Since in this case the deployed LLM on Azure AI is a coding-specific LLM, the agent will be created with `smolagents.CodeAgent` that adds the relevant prompt and parsing functionality, so as to interpret the LLM outputs as code. Alternatively, one could also use `smolagents.ToolCallingAgent` which is a tool calling agent, meaning that the given LLM should have tool calling capabilities.\n",
    "\n",
    "Then, the `smolagents.CodeAgent` expects both the `model` and the set of `tools` that the model has access to, and then via the `run` method, you can leverage all the potential of the agent in an automatic way, without manual intervention; so that the agent will use the given tools if needed, to answer or comply with your initial request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055566df-8efb-4ba3-8341-481f1bf4250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import CodeAgent\n",
    "\n",
    "agent = CodeAgent(\n",
    "    tools=[\n",
    "        get_time_in_timezone,\n",
    "        search_wikipedia,\n",
    "    ],\n",
    "    model=model,\n",
    "    stream_outputs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a23b75f-6657-4a2d-b6bc-705f0e096070",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\n",
    "    \"Could you create a Python function that given the summary of 'What is a Lemur?'\"\n",
    "    \" replaces all the occurrences of the letter E with the letter U (ignore the casing)\"     \n",
    ")\n",
    "# Summary for Lumur: Lumurs aru wut-nosud primatus of thu supurfamily Lumuroidua, dividud into 8 familius and consisting of 15 gunura and around 100 uxisting spucius. Thuy aru undumic to thu island of Madagascar. Most uxisting lumurs aru small, with a pointud snout, largu uyus, and a long tail. Thuy chiufly livu in truus and aru activu at night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bc630f-f70e-4537-a64e-669d2c0b71d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\n",
    "    \"What time is in Thailand right now? And what's the time difference with France?\"     \n",
    ")\n",
    "# The current time in Thailand is 5 hours ahead of the current time in France."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf20842-e691-4f8c-949c-fb3fc1038336",
   "metadata": {},
   "source": [
    "## Release resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0309e3d0-c871-43c7-bd43-73e0fd1a7277",
   "metadata": {},
   "source": [
    "Once you are done using the Foundry Endpoint, you can delete the resources (i.e., you will stop paying for the instance on which the model is running and all the attached costs) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e1e5f0-85d3-47b4-9a02-7b009ae2c7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.online_endpoints.begin_delete(name=os.getenv(\"ENDPOINT_NAME\")).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa2816f-bae1-48e4-8b4d-753b34a149ac",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d01c0b-44fc-40b6-bfc7-7ba8dcb1432f",
   "metadata": {},
   "source": [
    "Throughout this example you learnt how to deploy an Azure Machine Learning Managed Online Endpoint on Microsoft Foundry running an open model from the Hugging Face collection, leverage it to build agents with `smolagents`, and finally, how to stop and release the resources.\n",
    "\n",
    "If you have any doubt, issue or question about this example, feel free to [open an issue](https://github.com/huggingface/Microsoft-Azure/issues/new) and we'll do our best to help!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
