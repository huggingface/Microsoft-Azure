{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa3b3fb3-ccce-4654-8a3a-02bb4016eeaf",
   "metadata": {},
   "source": [
    "# Deploy Meta SAM 3 on Microsoft Foundry\n",
    "\n",
    "<div style=\"text-align: left;\">\n",
    "    <span style=\"display: inline-flex; align-items: center;\">\n",
    "        <img src=\"https://az-icons.com/api/icon/ai-studio/download?format=svg\" alt=\"Microsoft Foundry Logo\" style=\"height:1em; width:auto; margin-right:0.3em;\">\n",
    "        <a href=\"https://ai.azure.com/explore/models/facebook-sam3/version/2/registry/HuggingFace\">Meta SAM 3 on Microsoft Foundry</a>\n",
    "    </span>\n",
    "    •\n",
    "    <span style=\"display: inline-flex; align-items: center;\">\n",
    "        <img src=\"https://az-icons.com/api/icon/machine-learning/download?format=svg\" alt=\"Azure Machine Learning Logo\" style=\"height:1em; width:auto; margin-right:0.3em;\">\n",
    "        <a href=\"https://ml.azure.com/models/facebook-sam3/version/2/catalog/registry/HuggingFace\">Meta SAM 3 on Azure Machine Learning</a>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba7ccad-be43-4cdc-9aa3-ab5acd5b5899",
   "metadata": {},
   "source": [
    "> [!WARNING]\n",
    "> At the moment gated models can only be deployed programmatically, since the Microsoft Foundry hasn't incorporated those yet; meaning that when deploying from Microsoft Foundry or Azure Machine Learning, even if the `HuggingFaceTokenConnection` is set, you might stumble upon the following error:\n",
    "> ```\n",
    "> Failed to retrieve and inject workspace connection secrets. Please verify that the endpoint identity has been granted the Workspace Connection Secrets Reader role or any custom roles with actions Microsoft.MachineLearningServices/workspaces/connections/listsecrets/action & Microsoft.MachineLearningServices/workspaces/metadata/secrets/read and ensure that the secret reference schema in the environment variables is accurate.\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975fe589-9800-4124-a3be-d9f618da61a4",
   "metadata": {},
   "source": [
    "This example shows how to deploy Segment Anything Model 3 (SAM 3) from the Hugging Face collection on Microsoft Foundry (formerly Azure AI Foundry) as an Azure Machine Learning Managed Online Endpoint. SAM 3 is state-of-the-art across all text and visual segmentation tasks in both images and videos whilst maintaining all the performance and functionality of it predecessor, SAM 2.\n",
    "\n",
    "![SAM 3 on Microsoft Foundry](./sam3-microsoft-foundry.png)\n",
    "\n",
    "[SAM 3](https://huggingface.co/facebook/sam3) is a unified foundation model for promptable segmentation in images and videos, state-of-the-art in [`mask-generation`](https://huggingface.co/tasks/mask-generation). SAM 3 can detect, segment, and track objects using text or visual prompts such as points, boxes, and masks. Compared to its predecessor SAM 2, SAM 3 introduces the ability to exhaustively segment all instances of an open-vocabulary concept specified by a short text phrase or exemplars. Unlike prior work, SAM 3 can handle a vastly larger set of open-vocabulary prompts. It achieves 75-80% of human performance on our new SA-CO benchmark which contains 270K unique concepts, over 50 times more than existing benchmarks.\n",
    "\n",
    "![Meta SAM 3 Benchmarks](https://lookaside.fbsbx.com/elementpath/media/?media_id=1132635808951855&version=1763568008&transcode_extension=webp)\n",
    "\n",
    "[Mask generation](https://huggingface.co/tasks/mask-generation) is the task of generating masks that identify a specific object or region of interest in a given image. Masks are often used in segmentation tasks, where they provide a precise way to isolate the object of interest for further processing or analysis.\n",
    "\n",
    "For more information, make sure to check [Meta SAM 3 on the Hugging Face Hub](https://huggingface.co/facebook/sam3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763a3a3a-aae1-43be-a925-0a4e1f5493a3",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0800470a-050e-4948-b0b2-f6bc3a937e09",
   "metadata": {},
   "source": [
    "To run the following example, you will need to comply with the following pre-requisites, alternatively, you can also read more about those in the [Azure Machine Learning Tutorial: Create resources you need to get started](https://learn.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources?view=azureml-api-2).\n",
    "\n",
    "- An Azure account with an active subscription.\n",
    "- The Azure CLI installed and logged in.\n",
    "- The Azure Machine Learning extension for the Azure CLI.\n",
    "- An Azure Resource Group.\n",
    "- A Hub-based project on Microsoft Foundry (classic, i.e., Azure AI Foundry Hub-based project).\n",
    "\n",
    "For more information, please go through the steps in [Configure Azure Machine Learning and Microsoft Foundry](https://huggingface.co/docs/microsoft-azure/guides/configure-azure-ml-microsoft-foundry)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7262d0-ee10-4ffd-8104-6ff063ff527e",
   "metadata": {},
   "source": [
    "Additionally, given that [`facebook/sam3`](https://huggingface.co/facebook/sam3) is a gated model, you need to agree to the gating on the Hugging Face Hub and wait for the authors to approve it so that you can download the weights.\n",
    "\n",
    "![Meta SAM 3 gating on the Hub](./sam3-gating.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d439f949-f482-4ed9-9d66-0d6ae93d5173",
   "metadata": {},
   "source": [
    "## Setup and installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eade47-ccfa-4add-a10f-933e2190f169",
   "metadata": {},
   "source": [
    "In this example, the [Azure Machine Learning SDK for Python](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/ml/azure-ai-ml) will be used to create the endpoint and the deployment, as well as to invoke the deployed API. Along with it, you will also need to install `azure-identity` to authenticate with your Azure credentials via Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3979198-34e9-48f1-99f1-2c49b447a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-ml azure-identity --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026d4738-831e-4de0-a1df-dcacd05db5b8",
   "metadata": {},
   "source": [
    "More information at [Azure Machine Learning SDK for Python](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-ml-readme?view=azure-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351ec9b-3dbc-4227-82b6-0c71d47a6358",
   "metadata": {},
   "source": [
    "Then, setting the following environment variables is recommended as those will be used along the example for the Azure ML Client, so make sure to update and set those values accordingly as per your Microsoft Azure account and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5670d3e2-acbb-435b-b851-bca960a2c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env LOCATION eastus\n",
    "%env SUBSCRIPTION_ID <YOUR_SUBSCRIPTION_ID>\n",
    "%env RESOURCE_GROUP <YOUR_RESOURCE_GROUP>\n",
    "%env WORKSPACE_NAME <YOUR_WORKSPACE_NAME>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c2f65-1a8b-419e-b206-5e6f5d9a029d",
   "metadata": {},
   "source": [
    "You also need to define both the endpoint and deployment names, as those will be used throughout the example too:\n",
    "\n",
    "> [!NOTE]\n",
    "> Note that endpoint names must to be globally unique per region i.e., even if you don't have any endpoint named that way running under your subscription, if the name is reserved by another Azure customer, then you won't be able to use the same name. Adding a timestamp or a custom identifier is recommended to prevent running into HTTP 400 validation issues when trying to deploy an endpoint with an already locked / reserved name. Also the endpoint name must be between 3 and 32 characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e071302-9ea1-43ba-a0a9-8431615f3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from uuid import uuid4\n",
    "\n",
    "os.environ[\"ENDPOINT_NAME\"] = f\"endpoint-{str(uuid4())[:8]}\"\n",
    "os.environ[\"DEPLOYMENT_NAME\"] = f\"deployment-{str(uuid4())[:8]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028db6e-5a51-4611-aebf-42f7faffc603",
   "metadata": {},
   "source": [
    "Finally, as [`facebook/sam3`](https://huggingface.co/facebook/sam3) is a gated model, you need to create an Azure Machine Learning Connection with Custom keys named `HuggingFaceTokenConnection` with a key `HF_TOKEN` with [your Hugging Face read or fine-grained token](https://huggingface.co/settings/tokens/new?canReadGatedRepos=true&tokenType=fineGrained) flagged as secret to make sure that the model weights can be downloaded from the Hugging Face Hub on runtime.\n",
    "\n",
    "![Azure Machine Learning Connection for `HF_TOKEN`](./azureml-connection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b36389-76b9-4276-a751-1a52181833b9",
   "metadata": {},
   "source": [
    "Alternatively, you can also create the Azure Machine Learning Connection programmatically as follows:\n",
    "\n",
    "```bash\n",
    "az ml connection create \\\n",
    "    --name HuggingFaceTokenConnection \\\n",
    "    --type \"Generic\" \\\n",
    "    --resource-group $RESOURCE_GROUP \\\n",
    "    --workspace-name $WORKSPACE_NAME \\\n",
    "    --secret \"HF_TOKEN=<YOUR_HF_TOKEN_HERE>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e1c5f-4815-43b7-9227-f9b7c333329d",
   "metadata": {},
   "source": [
    "## Authenticate to Azure Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026488f0-3a44-433e-b84f-76588678ed77",
   "metadata": {},
   "source": [
    "First you need to authenticate into the Microsoft Foundry via Azure Machine Learning with the Python SDK:\n",
    "\n",
    "> [!NOTE]\n",
    "> On standard Azure Machine Learning deployments you'd need to create the `MLClient` using the Azure Machine Learning Workspace as the `workspace_name` whereas for Microsoft Foundry, you need to provide the Azure AI Foundry Hub-based project name as the `workspace_name` instead, and that will deploy the endpoint under Microsoft Foundry too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe4b31f-34b2-4209-ba99-f3ee99ce2c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "client = MLClient(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    subscription_id=os.getenv(\"SUBSCRIPTION_ID\"),\n",
    "    resource_group_name=os.getenv(\"RESOURCE_GROUP\"),\n",
    "    workspace_name=os.getenv(\"WORKSPACE_NAME\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2498c42-2e9a-4d0f-9c51-2d9775a92eaf",
   "metadata": {},
   "source": [
    "## Create and Deploy Foundry Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263d2cf5-f4c0-4da7-9d3d-0e7f1bf991c9",
   "metadata": {},
   "source": [
    "To create the Azure Machine Learning Managed Online Endpoint you don't need to provide the model ID on the Hugging Face Hub but rather the model URI on Azure Machine Learning formatted as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a77a0-3ce3-418d-b161-757a2b09232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"facebook/sam3\"\n",
    "\n",
    "model_uri = f\"azureml://registries/HuggingFace/models/{model_id.replace('/', '-').replace('_', '-').lower()}/labels/latest\"\n",
    "model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbe1093-1ede-4d6b-8cfe-4fec548dcd85",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> To check if a model from the Hugging Face Hub is available in Azure, you should read about it in [Supported Models](https://huggingface.co/docs/microsoft-azure/azure-ai/models). If not, you can always [Request a model addition in the Hugging Face collection on Azure](https://huggingface.co/docs/microsoft-azure/guides/request-model-addition))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13973dce-4a76-40e9-a344-b6fca002eb24",
   "metadata": {},
   "source": [
    "Then you need to create the [ManagedOnlineEndpoint via the Azure ML Python SDK](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.managedonlineendpoint?view=azure-python) making sure to include the `enfoce_access_to_default_secret_stores: enabled` property so that the Azure Machine Learning Connection secret can be read to later pull the weights from the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e8cd42-5ea4-44a7-b998-dbd1cbf9c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import ManagedOnlineEndpoint\n",
    "\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=os.getenv(\"ENDPOINT_NAME\"),\n",
    "    properties={\"enforce_access_to_default_secret_stores\": \"enabled\"},\n",
    ")\n",
    "\n",
    "client.begin_create_or_update(endpoint).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8ca4f3-cf37-45f2-b16e-7ac11b27e8bc",
   "metadata": {},
   "source": [
    "After creating the endpoint, you need to create the [ManagedOnlineDeployment via the Azure ML Python SDK](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.managedonlinedeployment?view=azure-python) i.e., create a deployment linked to the given endpoint.\n",
    "\n",
    "> [!NOTE]\n",
    "> All Hugging Face models on Azure run on an optimized inference backend that supports multiple hardware SKUs, as listed in the [Supported Hardware page](https://huggingface.co/docs/microsoft-azure/azure-ai/supported-hardware). Some models require GPU-enabled instances, so you may need to request a quota increase using the [Azure Machine Learning quota management guide](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-quotas?view=azureml-api-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d81645-0a8b-4388-b250-13564ea86b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import ManagedOnlineDeployment\n",
    "\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    name=os.getenv(\"DEPLOYMENT_NAME\"),\n",
    "    endpoint_name=os.getenv(\"ENDPOINT_NAME\"),\n",
    "    model=model_uri,\n",
    "    instance_type=\"Standard_NC40ads_H100_v5\",\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "client.online_deployments.begin_create_or_update(deployment).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1315d858-e7dd-4475-b822-a89d11d340fe",
   "metadata": {},
   "source": [
    "The deployment might take ~10-15 minutes, but it could as well take longer depending on the selected SKU availability in the region. Once deployed, you will be able to inspect the endpoint details, the real-time logs, how to consume the endpoint, and [monitoring (on preview)](https://learn.microsoft.com/en-us/azure/machine-learning/concept-model-monitoring?view=azureml-api-2).\n",
    "\n",
    "Find more information about it at [Azure Machine Learning Managed Online Endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints-online?view=azureml-api-2#managed-online-endpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614189ee-a224-404a-bd5a-2347d7404c5f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Send requests to the Foundry Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9b306-7331-4fc3-bb65-d7e2134e7d5e",
   "metadata": {},
   "source": [
    "Once the Foundry Deployment is done, you can send requests to the Foundry Endpoint following [the API specification defined in the model card on Microsoft Foundry](https://ai.azure.com/catalog/models/facebook-sam3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca81a635-c81b-4bd8-80c4-d2b8b2680d8f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Azure Machine Learning SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca56b5-e66e-444e-934f-c6c757adb4b9",
   "metadata": {},
   "source": [
    "The Foundry Endpoint can be invoked via the Azure ML Client previously instantiated, but it requires an actual JSON file, which means that you first need to dump the payload into a JSON file then invoke the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6483ccfc-1d49-461e-b477-e583c837b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(mode=\"w+\", delete=True, suffix=\".json\") as f:\n",
    "    json.dump({\n",
    "        \"inputs\": \"http://images.cocodataset.org/val2017/000000077595.jpg\",\n",
    "        \"parameters\": {\n",
    "            \"points_per_batch\":16,\n",
    "        }\n",
    "    }, f)\n",
    "    \n",
    "    f.flush()\n",
    "\n",
    "    response = client.online_endpoints.invoke(\n",
    "        endpoint_name=os.getenv(\"ENDPOINT_NAME\"),\n",
    "        deployment_name=os.getenv(\"DEPLOYMENT_NAME\"),\n",
    "        request_file=f.name,\n",
    "    )\n",
    "    output = json.loads(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1bf107-5d89-4d60-a268-34552dd9b7ed",
   "metadata": {},
   "source": [
    "The `output` will contain a key `results` with a list of all the generated masks, meaning each item in the list is a dict with the keys `mask`, with the generated mask, and `score`, with the generated score for the given mask; note that the results are sorted by `score` from higher to lower (in the 0.0 to 1.0 range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cf10dd-a11e-4bb8-b2eb-157fa064ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"results\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de78c8-f556-4159-a349-bcb2b124c0d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Python `requests`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c67283-b161-4a64-9a57-583ecc22bb85",
   "metadata": {},
   "source": [
    "Alternatively, you can also send standard HTTP requests via e.g. cURL or Python `requests`, but there's a few things to take into consideration:\n",
    "\n",
    "- The API URL is available via `client.online_endpoints.get(os.getenv(\"ENDPOINT_NAME\")).scoring_uri`, or rather via Microsoft Foundry or Azure Machine Learning.\n",
    "- The API Key needs to be provided as a Bearer token in the authorization header and is available via `client.online_endpoints.get_keys(os.getenv(\"ENDPOINT_NAME\")).primary_key`, or rather via Microsoft Foundry or Azure Machine Learning.\n",
    "- The header `azureml-model-deployment` with `os.getenv(\"DEPLOYMENT_NAME\")` needs to be provided given that an endpoint can have more than one deployment, hence its name needs to be provided as a header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fd7936-9512-4f68-8689-34653bdd899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = client.online_endpoints.get(os.getenv(\"ENDPOINT_NAME\")).scoring_uri\n",
    "key = client.online_endpoints.get_keys(os.getenv('ENDPOINT_NAME')).primary_key\n",
    "\n",
    "output = requests.post(\n",
    "    url,\n",
    "    headers={\n",
    "        \"Authorization\": f\"Bearer {key}\",\n",
    "        \"azureml-model-deployment\": os.getenv(\"DEPLOYMENT_NAME\"),\n",
    "    },\n",
    "    data={\n",
    "        \"inputs\":\"http://images.cocodataset.org/val2017/000000077595.jpg\",\n",
    "        \"parameters\": { \"points_per_batch\":16 },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a397ef8-5e65-4ea6-9ff6-300c9de98c8a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Print generated masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab988b20-1376-46ec-bf17-67b7483afbcd",
   "metadata": {},
   "source": [
    "Once the masks have been generated, you can convert those into `PIL` images given that those are generated encoded in base64; then you can print those (not the masks are \"boolean\" images hence black and white) with the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f579dd9b-2f74-4a1e-8f01-3b1446d0726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy Pillow matplotlib --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506a6c7c-5880-4d21-ac51-a45ed7deeb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import math\n",
    "from io import BytesIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "mask_size = (128, 128)\n",
    "spacing = 24\n",
    "\n",
    "pil_masks = []\n",
    "for result in output[\"results\"]:\n",
    "    mask_b64 = result[\"mask\"]\n",
    "    mask_bytes = base64.b64decode(mask_b64)\n",
    "    mask_img = Image.open(BytesIO(mask_bytes)).convert(\"L\").resize(mask_size)\n",
    "    pil_masks.append(mask_img)\n",
    "\n",
    "n = len(pil_masks)\n",
    "cols = min(4, n)\n",
    "rows = math.ceil(n / cols)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols * 2.2, rows * 2.2))\n",
    "axes = np.atleast_1d(axes).flatten()\n",
    "\n",
    "for idx, ax in enumerate(axes):\n",
    "    if idx < n:\n",
    "        ax.imshow(pil_masks[idx], cmap=\"gray\")\n",
    "    else:\n",
    "        ax.imshow(np.zeros(mask_size), cmap=\"gray\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_color(\"lightgray\")\n",
    "        spine.set_linewidth(spacing // 8)\n",
    "\n",
    "plt.subplots_adjust(wspace=spacing / mask_size[0], hspace=spacing / mask_size[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a73af05-83ee-44c4-90d1-249f6a32f944",
   "metadata": {},
   "source": [
    "![Grid of black and white generated masks](./cat-masks-only.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b83e48b-8cee-4f65-810c-216fd178015b",
   "metadata": {},
   "source": [
    "As a recommendation, to better see the generated masks you can print those over the generated image with a different color with the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a338590-5716-45f4-b371-4c1d6e48a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://images.cocodataset.org/val2017/000000077595.jpg -O image.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff24ec87-4bb1-41ad-b1b6-47a464b211a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import math\n",
    "from io import BytesIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "image = np.array(Image.open(\"image.jpg\").convert(\"RGB\"))\n",
    "\n",
    "def mask_from_base64(mask_b64, target_shape):\n",
    "    mask_bytes = base64.b64decode(mask_b64)\n",
    "    mask_img = (\n",
    "        Image.open(BytesIO(mask_bytes))\n",
    "        .convert(\"L\")\n",
    "        .resize((target_shape[1], target_shape[0]), resample=Image.NEAREST)\n",
    "    )\n",
    "    return np.array(mask_img) > 0\n",
    "\n",
    "mask_list = [\n",
    "    mask_from_base64(res[\"mask\"], image.shape[:2]) for res in output[\"results\"]\n",
    "]\n",
    "\n",
    "n = len(mask_list) + 1\n",
    "cols = min(4, n)\n",
    "rows = math.ceil(n / cols)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))\n",
    "axes = np.atleast_1d(axes).flatten()\n",
    "\n",
    "for idx, mask in enumerate(mask_list):\n",
    "    overlay = image.copy()\n",
    "    color = np.random.randint(0, 256, 3, dtype=np.uint8)\n",
    "    alpha = 0.5\n",
    "    overlay[mask] = (alpha * color + (1 - alpha) * overlay[mask]).astype(np.uint8)\n",
    "    axes[idx].imshow(overlay)\n",
    "    axes[idx].set_title(f\"Mask {idx}\")\n",
    "\n",
    "for ax in axes[n:]:\n",
    "    ax.axis(\"off\")\n",
    "for ax in axes[:n]:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadb9b39-c780-410b-8b07-1aa68405154f",
   "metadata": {},
   "source": [
    "![Grid of original image with generated masks overlay](./cat-with-masks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf20842-e691-4f8c-949c-fb3fc1038336",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Release resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c475ca0a-cea2-45bc-b93c-3824fc73cf6d",
   "metadata": {},
   "source": [
    "Once you are done using the Foundry Endpoint, you can delete the resources (i.e., you will stop paying for the instance on which the model is running and all the attached costs) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd52337-cce6-431c-b036-fb91d0a894ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.online_endpoints.begin_delete(name=os.getenv(\"ENDPOINT_NAME\")).result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
