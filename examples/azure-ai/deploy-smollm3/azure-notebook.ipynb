{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa3b3fb3-ccce-4654-8a3a-02bb4016eeaf",
   "metadata": {},
   "source": [
    "# Deploy SmolLM3 on Azure AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975fe589-9800-4124-a3be-d9f618da61a4",
   "metadata": {},
   "source": [
    "This example showcases how to deploy SmolLM3 from the Hugging Face Collection in Azure AI Foundry Hub as an Azure ML Managed Online Endpoint, powered by Transformers with an OpenAI compatible route. Additionally, this example also showcases how to run inference with the OpenAI Python SDK for different scenarios and use-cases.\n",
    "\n",
    "![SmolLM3 3B Logo image](https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/zy0dqTCCt5IHmuzwoqtJ9.png)\n",
    "\n",
    "TL;DR Transformers acts as the model-definition framework for state-of-the-art machine learning models in text, computer vision, audio, video, and multimodal model, for both inference and training. Azure AI Foundry provides a unified platform for enterprise AI operations, model builders, and application development. Azure Machine Learning is a cloud service for accelerating and managing the machine learning (ML) project lifecycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d723dca0-9016-4b7c-a21b-ff01f75ea440",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21e077e",
   "metadata": {},
   "source": [
    "This example will specifically deploy [`HuggingFaceTB/SmolLM3-3B`](https://huggingface.co/HuggingFaceTB/SmolLM3-3B) from the Hugging Face Hub (or see it on [AzureML](https://ml.azure.com/models/huggingfacetb-smollm3-3b/version/3/catalog/registry/HuggingFace) or on [Azure AI Foundry](https://ai.azure.com/explore/models/huggingfacetb-smollm3-3b/version/3/registry/HuggingFace)) as an Azure ML Managed Online Endpoint on Azure AI Foundry Hub.\n",
    "\n",
    "SmolLM3 is a 3B parameter language model designed to push the boundaries of small models. It supports dual mode reasoning, 6 languages and long context. SmolLM3 is a fully open model that offers strong performance at the 3Bâ€“4B scale.\n",
    "\n",
    "![Small LLM win-rate on benchmarks per model size](https://cdn-uploads.huggingface.co/production/uploads/6200d0a443eb0913fa2df7cc/db3az7eGzs-Sb-8yUj-ff.png)\n",
    "\n",
    "The model is a decoder-only transformer using GQA and NoPE (with 3:1 ratio), it was pretrained on 11.2T tokens with a staged curriculum of web, code, math and reasoning data. Post-training included midtraining on 140B reasoning tokens followed by supervised fine-tuning and alignment via Anchored Preference Optimization (APO).\n",
    "\n",
    "- Instruct model optimized for **hybrid reasoning**\n",
    "- **Fully open model**: open weights + full training details including public data mixture and training configs\n",
    "- **Long context:** Trained on 64k context and suppots up to **128k tokens** using YARN extrapolation\n",
    "- **Multilingual**: 6 natively supported (English, French, Spanish, German, Italian, and Portuguese)\n",
    "\n",
    "![SmolLM3 3B on the Hugging Face Hub](./smollm3-hub.png)\n",
    "\n",
    "![SmolLM3 3B on Azure AI Foundry](./smollm3-azure-ai.png)\n",
    "\n",
    "For more information, make sure to check [our model card on the Hugging Face Hub](https://huggingface.co/HuggingFaceTB/SmolLM3-3B/blob/main/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763a3a3a-aae1-43be-a925-0a4e1f5493a3",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0800470a-050e-4948-b0b2-f6bc3a937e09",
   "metadata": {},
   "source": [
    "To run the following example, you will need to comply with the following pre-requisites, alternatively, you can also read more about those in the [Azure Machine Learning Tutorial: Create resources you need to get started](https://learn.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources?view=azureml-api-2).\n",
    "\n",
    "- An Azure account with an active subscription.\n",
    "- The Azure CLI installed and logged in.\n",
    "- The Azure Machine Learning extension for the Azure CLI.\n",
    "- An Azure Resource Group.\n",
    "- A project based on an Azure AI Foundry Hub.\n",
    "\n",
    "For more information, please go through the steps in [Configure Microsoft Azure for Azure AI](https://huggingface.co/docs/microsoft-azure/azure-ai/configure)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d439f949-f482-4ed9-9d66-0d6ae93d5173",
   "metadata": {},
   "source": [
    "## Setup and installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eade47-ccfa-4add-a10f-933e2190f169",
   "metadata": {},
   "source": [
    "In this example, the [Azure Machine Learning SDK for Python](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/ml/azure-ai-ml) will be used to create the endpoint and the deployment, as well as to invoke the deployed API. Along with it, you will also need to install `azure-identity` to authenticate with your Azure credentials via Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3979198-34e9-48f1-99f1-2c49b447a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-ml azure-identity --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026d4738-831e-4de0-a1df-dcacd05db5b8",
   "metadata": {},
   "source": [
    "More information at [Azure Machine Learning SDK for Python](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-ml-readme?view=azure-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351ec9b-3dbc-4227-82b6-0c71d47a6358",
   "metadata": {},
   "source": [
    "Then, for convenience setting the following environment variables is recommended as those will be used along the example for the Azure ML Client, so make sure to update and set those values accordingly as per your Microsoft Azure account and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5670d3e2-acbb-435b-b851-bca960a2c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env LOCATION eastus\n",
    "%env SUBSCRIPTION_ID <YOUR_SUBSCRIPTION_ID>\n",
    "%env RESOURCE_GROUP <YOUR_RESOURCE_GROUP>\n",
    "%env AI_FOUNDRY_HUB_PROJECT <YOUR_AI_FOUNDRY_HUB_PROJECT>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c2f65-1a8b-419e-b206-5e6f5d9a029d",
   "metadata": {},
   "source": [
    "Finally, you also need to define both the endpoint and deployment names, as those will be used throughout the example too:\n",
    "\n",
    "> [!NOTE]\n",
    "> Note that endpoint names must to be globally unique per region i.e., even if you don't have any endpoint named that way running under your subscription, if the name is reserved by another Azure customer, then you won't be able to use the same name. Adding a timestamp or a custom identifier is recommended to prevent running into HTTP 400 validation issues when trying to deploy an endpoint with an already locked / reserved name. Also the endpoint name must be between 3 and 32 characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e071302-9ea1-43ba-a0a9-8431615f3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from uuid import uuid4\n",
    "\n",
    "os.environ[\"ENDPOINT_NAME\"] = f\"smollm3-endpoint-{str(uuid4())[:8]}\"\n",
    "os.environ[\"DEPLOYMENT_NAME\"] = f\"smollm3-deployment-{str(uuid4())[:8]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e1c5f-4815-43b7-9227-f9b7c333329d",
   "metadata": {},
   "source": [
    "## Authenticate to Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026488f0-3a44-433e-b84f-76588678ed77",
   "metadata": {},
   "source": [
    "Initially, you need to authenticate into the Azure AI Foundry Hub via Azure ML with the Azure ML Python SDK, which will be later used to deploy `HuggingFaceTB/SmolLM3-3B` as an Azure ML Managed Online Endpoint in your Azure AI Foundry Hub.\n",
    "\n",
    "> [!NOTE]\n",
    "> On standard Azure ML deployments you'd need to create the `MLClient` using the Azure ML Workspace as the `workspace_name` whereas for Azure AI Foundry, you need to provide the Azure AI Foundry Hub name as the `workspace_name` instead, and that will deploy the endpoint under the Azure AI Foundry too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe4b31f-34b2-4209-ba99-f3ee99ce2c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "client = MLClient(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    subscription_id=os.getenv(\"SUBSCRIPTION_ID\"),\n",
    "    resource_group_name=os.getenv(\"RESOURCE_GROUP\"),\n",
    "    workspace_name=os.getenv(\"AI_FOUNDRY_HUB_PROJECT\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2498c42-2e9a-4d0f-9c51-2d9775a92eaf",
   "metadata": {},
   "source": [
    "## Create and Deploy Azure AI Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263d2cf5-f4c0-4da7-9d3d-0e7f1bf991c9",
   "metadata": {},
   "source": [
    "Before creating the Managed Online Endpoint, you need to build the model URI, which is formatted as it follows `azureml://registries/HuggingFace/models/<MODEL_ID>/labels/latest` where the `MODEL_ID` won't be the Hugging Face Hub ID but rather its name on Azure, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a77a0-3ce3-418d-b161-757a2b09232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"HuggingFaceTB/SmolLM3-3B\"\n",
    "\n",
    "model_uri = f\"azureml://registries/HuggingFace/models/{model_id.replace('/', '-').replace('_', '-').lower()}/labels/latest\"\n",
    "model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbe1093-1ede-4d6b-8cfe-4fec548dcd85",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> To check if a model from the Hugging Face Hub is available in Azure, you should read about it in [Supported Models](https://huggingface.co/docs/microsoft-azure/azure-ai/models). If not, you can always [Request a model addition in the Hugging Face collection on Azure](https://huggingface.co/docs/microsoft-azure/guides/request-model-addition))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13973dce-4a76-40e9-a344-b6fca002eb24",
   "metadata": {},
   "source": [
    "Then you need to create the [ManagedOnlineEndpoint via the Azure ML Python SDK](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.managedonlineendpoint?view=azure-python) as follows.\n",
    "\n",
    "> [!NOTE]\n",
    "> Every model in the Hugging Face Collection is powered by an efficient inference backend, and each of those can run on a wide variety of instance types (as listed in [Supported Hardware](https://huggingface.co/docs/microsoft-azure/azure-ai/supported-hardware)). Since for models and inference engines require a GPU-accelerated instance, you might need to request a quota increase as per [Manage and increase quotas and limits for resources with Azure Machine Learning](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-quotas?view=azureml-api-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e8cd42-5ea4-44a7-b998-dbd1cbf9c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment\n",
    "\n",
    "endpoint = ManagedOnlineEndpoint(name=os.getenv(\"ENDPOINT_NAME\"))\n",
    "\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    name=os.getenv(\"DEPLOYMENT_NAME\"),\n",
    "    endpoint_name=os.getenv(\"ENDPOINT_NAME\"),\n",
    "    model=model_uri,\n",
    "    instance_type=\"Standard_NC40ads_H100_v5\",\n",
    "    instance_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d81645-0a8b-4388-b250-13564ea86b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.begin_create_or_update(endpoint).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646eb1ae-1497-4303-9631-e5bab3022301",
   "metadata": {},
   "source": [
    "![Azure AI Endpoint from Azure AI Foundry](./azure-ai-endpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce20ded-35ee-49f8-a8bd-1f6fd25db162",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> In Azure AI Foundry the endpoint will only be listed within the \"My assets -> Models + endpoints\" tab once the deployment is created, not before as in Azure ML where the endpoint is shown even if it doesn't contain any active or in-progress deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e82cd-3bd4-4ecf-a5bf-6423b37733ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.online_deployments.begin_create_or_update(deployment).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b135caa-a700-4e5c-8f2b-48b9a709c8b2",
   "metadata": {},
   "source": [
    "![Azure AI Deployment from Azure AI Foundry](./azure-ai-deployment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54884da-0413-438c-8f76-b22302149e4d",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> Note that whilst the Azure AI Endpoint creation is relatively fast, the deployment will take longer since it needs to allocate the resources on Azure so expect it to take ~10-15 minutes, but it could as well take longer depending on the instance provisioning and availability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1315d858-e7dd-4475-b822-a89d11d340fe",
   "metadata": {},
   "source": [
    "Once deployed, via either the Azure AI Foundry or the Azure ML Studio you'll be able to inspect the endpoint details, the real-time logs, how to consume the endpoint, and even use the, still on preview, [monitoring feature](https://learn.microsoft.com/en-us/azure/machine-learning/concept-model-monitoring?view=azureml-api-2). Find more information about it at [Azure ML Managed Online Endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints-online?view=azureml-api-2#managed-online-endpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614189ee-a224-404a-bd5a-2347d7404c5f",
   "metadata": {},
   "source": [
    "## Send requests to the Azure AI Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9b306-7331-4fc3-bb65-d7e2134e7d5e",
   "metadata": {},
   "source": [
    "Finally, now that the Azure AI Endpoint is deployed, you can send requests to it. In this case, since the task of the model is `text-generation` (also known as `chat-completion`) you can use the OpenAI SDK with the OpenAI-compatible route and send requests to the scoring URI i.e., `/v1/chat/completions`.\n",
    "\n",
    "> [!NOTE]\n",
    "> Note that below only some of the options are listed, but you can send requests to the deployed endpoint as long as you send the HTTP requests with the `azureml-model-deployment` header set to the name of the Azure AI Deployment (not the Endpoint), and have the necessary authentication token / key to send requests to the given endpoint; then you can send HTTP request to all the routes that the backend engine is exposing, not only to the scoring route.\n",
    "\n",
    "> [!WARNING]\n",
    "> Support for Hugging Face models via [`azure-ai-inference` Python SDK](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/ai/azure-ai-inference) is still a work in progress, but that will be included soon and set as the recommended inference method, stay tuned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37703050-df09-4825-9328-b591ef53632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c2bfad-a1e8-483f-888b-2793097fb01a",
   "metadata": {},
   "source": [
    "To use the OpenAI Python SDK with Azure ML Managed Online Endpoints, you need to first retrieve:\n",
    "\n",
    "- `api_url` with the `/v1` route (that contains the `v1/chat/completions` endpoint that the OpenAI Python SDK will send requests to)\n",
    "- `api_key` which is the API Key in Azure AI or the primary key in Azure ML (unless a dedicated Azure ML Token is used instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dddbe29-8498-46d5-bc71-6903cf8a36ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlsplit\n",
    "\n",
    "api_key = client.online_endpoints.get_keys(os.getenv(\"ENDPOINT_NAME\")).primary_key\n",
    "\n",
    "url_parts = urlsplit(client.online_endpoints.get(os.getenv(\"ENDPOINT_NAME\")).scoring_uri)\n",
    "api_url = f\"{url_parts.scheme}://{url_parts.netloc}/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29679b34-3982-4ed6-92a5-6136d596310d",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> Alternatively, you can also build the API URL manually as it follows, since the URIs are globally unique per region, meaning that there will only be one endpoint named the same way within the same region:\n",
    "> ```python\n",
    "> api_url = f\"https://{os.getenv('ENDPOINT_NAME')}.{os.getenv('LOCATION')}.inference.ml.azure.com/v1\"\n",
    "> ```\n",
    "> Or just retrieve it from either the Azure AI Foundry or the Azure ML Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1e2ff6-c135-43c2-b466-49c9054030ef",
   "metadata": {},
   "source": [
    "Then you can use the OpenAI Python SDK normally, making sure to include the extra header `azureml-model-deployment` header that contains the Azure AI / ML Deployment name.\n",
    "\n",
    "Via the OpenAI Python SDK it can either be set within each call to `chat.completions.create` via the `extra_headers` parameter as commented below, or via the `default_headers` parameter when instantiating the `OpenAI` client (which is the recommended approach since the header needs to be present on each request, so setting it just once is preferred)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cb6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI(\n",
    "    base_url=api_url,\n",
    "    api_key=api_key,\n",
    "    default_headers={\"azureml-model-deployment\": os.getenv(\"DEPLOYMENT_NAME\")},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27638ec",
   "metadata": {},
   "source": [
    "### Chat Completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa18d5-04f5-4876-820b-d416e91e588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai_client.chat.completions.create(\n",
    "    model=\"HuggingFaceTB/SmolLM3-3B\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an assistant that responds like a pirate.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Give me a brief explanation of gravity in simple terms.\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=128,\n",
    ")\n",
    "print(completion)\n",
    "# ChatCompletion(id='chatcmpl-74f6852e28', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=\"<think>\\nOkay, the user wants a simple explanation of gravity. Let me start by recalling what I know. Gravity is the force that pulls objects towards each other. But how to explain that simply?\\n\\nMaybe start with a common example, like how you fall when you jump. That's gravity pulling you down. But wait, I should mention that it's not just on Earth. The moon orbits the Earth because of gravity too. But how to make that easy to understand?\\n\\nI need to avoid technical terms. Maybe use metaphors. Like comparing gravity to a magnet, but not exactly. Or think of it as a stretchy rope pulling\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1753178803, model='HuggingFaceTB/SmolLM3-3B', object='chat.completion', service_tier='default', system_fingerprint='1a28be5c-df18-4e97-822f-118bf57374c8', usage=CompletionUsage(completion_tokens=128, prompt_tokens=66, total_tokens=194, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0), reasoning_tokens=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a12628",
   "metadata": {},
   "source": [
    "### Extended Thinking Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777b8c3e",
   "metadata": {},
   "source": [
    "By default, `SmolLM3-3B` enables extended thinking, so the example above generates the output with a reasoning trace as the reasoning is enabled by default.\n",
    "\n",
    "To enable and disable it, you can provide either `/think` and `/no_think` in the system prompt, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a86f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai_client.chat.completions.create(\n",
    "    model=\"HuggingFaceTB/SmolLM3-3B\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"/no_think You are an assistant that responds like a pirate.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Give me a brief explanation of gravity in simple terms.\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=128,\n",
    ")\n",
    "print(completion)\n",
    "# ChatCompletion(id='chatcmpl-776e84a272', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Arr matey! Ye be askin' about gravity, the mighty force that keeps us swabbin' the decks and not floatin' off into the vast blue yonder. Gravity be the pull o' the Earth, a mighty force that keeps us grounded and keeps the stars in their place. It's like a giant invisible hand that pulls us towards the center o' the Earth, makin' sure we don't float off into space. It's what makes the apples fall from the tree and the moon orbit 'round the Earth. So, gravity be the force that keeps us all tied to this fine planet we call home.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1753178805, model='HuggingFaceTB/SmolLM3-3B', object='chat.completion', service_tier='default', system_fingerprint='d644cb1c-84d6-49ae-b790-ac6011851042', usage=CompletionUsage(completion_tokens=128, prompt_tokens=72, total_tokens=200, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0), reasoning_tokens=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28038bd8",
   "metadata": {},
   "source": [
    "### Multilingual capabilities\n",
    "\n",
    "As mentioned before, `SmolLM3-3B` has been trained to natively suport 6 languages: English, French, Spanish, German, Italian, and Portuguese; meaning that you can leverage its multilingual potential by sending requests on any of those languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7fb086",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai_client.chat.completions.create(\n",
    "    model=\"HuggingFaceTB/SmolLM3-3B\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"/no_think You are an expert translator.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Translate the following English sentence into both Spanish and German: 'The brown cat sat on the mat.'\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=128,\n",
    ")\n",
    "print(completion)\n",
    "# ChatCompletion(id='chatcmpl-da6188629f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The translation of the English sentence 'The brown cat sat on the mat.' into Spanish is: 'El gato marrÃ³n se sentÃ³ en el tapete.'\\n\\nThe translation of the English sentence 'The brown cat sat on the mat.' into German is: 'Der braune Katze saÃŸ auf dem Teppich.'\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1753178807, model='HuggingFaceTB/SmolLM3-3B', object='chat.completion', service_tier='default', system_fingerprint='054f8a76-4e8c-4a2f-90eb-31f0e802916c', usage=CompletionUsage(completion_tokens=68, prompt_tokens=77, total_tokens=145, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0), reasoning_tokens=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf178d2",
   "metadata": {},
   "source": [
    "### Agentic use-cases and Tool Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9fbdbc",
   "metadata": {},
   "source": [
    "`SmolLM3-3B` has tool calling capabilities, meaning that you can provide a tool or list of tools that the LLM can leverage and use.\n",
    "\n",
    "> [!NOTE]\n",
    "> To prevent the `tool_call` from being incomplete, you might need either unset the value for `max_completion_tokens` (former `max_tokens`) or set it to a fair enough value so that the model stops producing tokens due to length limitations before the `tool_call` is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85749877",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai_client.chat.completions.create(\n",
    "    model=\"HuggingFaceTB/SmolLM3-3B\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the weather like in New York?\"}],\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        },\n",
    "                        \"unit\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                            \"description\": \"The unit of temperature\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    tool_choice=\"auto\",\n",
    "    max_completion_tokens=256,\n",
    ")\n",
    "print(completion)\n",
    "# ChatCompletion(id='chatcmpl-c36090e6b5', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content='<think>I need to retrieve the current weather information for New York, so I\\'ll use the get_weather function with the location set to \\'New York\\' and the unit set to \\'fahrenheit\\'.</think>\\n<tool_call>{\"name\": \"get_weather\", \"arguments\": {\"location\": \"New York\", \"unit\": \"fahrenheit\"}}</tool_call>', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call-5d5eb71a', function=Function(arguments='{\"location\": \"New York\", \"unit\": \"fahrenheit\"}', name='get_weather'), type='function')]))], created=1753178808, model='HuggingFaceTB/SmolLM3-3B', object='chat.completion', service_tier='default', system_fingerprint='5e58b305-773c-40b6-900b-fe5b177aeab9', usage=CompletionUsage(completion_tokens=68, prompt_tokens=442, total_tokens=510, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0), reasoning_tokens=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf20842-e691-4f8c-949c-fb3fc1038336",
   "metadata": {},
   "source": [
    "## Release resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c475ca0a-cea2-45bc-b93c-3824fc73cf6d",
   "metadata": {},
   "source": [
    "Once you are done using the Azure AI Endpoint / Deployment, you can delete the resources as it follows, meaning that you will stop paying for the instance on which the model is running and all the attached costs will be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd52337-cce6-431c-b036-fb91d0a894ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.online_endpoints.begin_delete(name=os.getenv(\"ENDPOINT_NAME\")).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa2816f-bae1-48e4-8b4d-753b34a149ac",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d01c0b-44fc-40b6-bfc7-7ba8dcb1432f",
   "metadata": {},
   "source": [
    "Throughout this example you learnt how to create and configure your Azure account for Azure ML and Azure AI Foundry, how to then create a Managed Online Endpoint running an open model from the Hugging Face Collection in the Azure ML / Azure AI Foundry model catalog, how to send inference requests with OpenAI SDK for a wide variety of use-cases, and finally, how to stop and release the resources.\n",
    "\n",
    "If you have any doubt, issue or question about this example, feel free to [open an issue](https://github.com/huggingface/Microsoft-Azure/issues/new) and we'll do our best to help!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
