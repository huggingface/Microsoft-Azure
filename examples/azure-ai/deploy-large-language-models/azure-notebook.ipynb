{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa3b3fb3-ccce-4654-8a3a-02bb4016eeaf",
   "metadata": {},
   "source": [
    "# Deploy Large Language Models (LLMs) on Azure AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975fe589-9800-4124-a3be-d9f618da61a4",
   "metadata": {},
   "source": [
    "This example showcases how to deploy a Large Language Model (LLM) from the Hugging Face Collection in Azure AI Foundry Hub as an Azure ML Managed Online Endpoint, powered by Hugging Face's Text Generation Inference (TGI). Additionally, this example also showcases how to run inference with both the Azure ML Python SDK, the OpenAI Python SDK, and even how to locally run a Gradio application for chat completion.\n",
    "\n",
    "> [!NOTE]\n",
    "> Note that this example will go through the Python SDK / Azure CLI programmatic deployment, if you'd rather prefer using the one-click deployment experience, please check [One-click deployments from the Hugging Face Hub on Azure ML](https://huggingface.co/docs/microsoft-azure/guides/one-click-deployment-azure-ml). But note that when deploying from the Hugging Face Hub, the endpoint + deployment will be created within Azure ML instead of within Azure AI Foundry, whereas this example focuses on Azure AI Foundry Hub deployments (also made available on Azure ML, but not the other way around).\n",
    "\n",
    "TL;DR Text Generation Inference (TGI) is a solution developed by Hugging Face for deploying and serving LLMs and VLMs with high performance text generation. Azure AI Foundry provides a unified platform for enterprise AI operations, model builders, and application development. Azure Machine Learning is a cloud service for accelerating and managing the machine learning (ML) project lifecycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d723dca0-9016-4b7c-a21b-ff01f75ea440",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef145992-e426-491d-b218-87ecd9d06d65",
   "metadata": {},
   "source": [
    "This example will specifically deploy [`Qwen/Qwen2.5-32B-Instruct`](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct) from the Hugging Face Hub (or see it on [AzureML](https://ml.azure.com/models/qwen-qwen2.5-32b-instruct/version/1/catalog/registry/HuggingFace) or on [Azure AI Foundry](https://ai.azure.com/explore/models/qwen-qwen2.5-32b-instruct/version/1/registry/HuggingFace)) as an Azure ML Managed Online Endpoint on Azure AI Foundry Hub.\n",
    "\n",
    "Qwen2.5 is one of the latest series of Qwen large language models, bringing the following improvements upon Qwen2 such as:\n",
    "\n",
    "- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n",
    "- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n",
    "- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n",
    "- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n",
    "\n",
    "![Qwen2.5 32B Instruct on the Hugging Face Hub](./qwen2.5-hub.png)\n",
    "\n",
    "<!-- ![Qwen2.5 32B Instruct on Azure ML](./qwen2.5-azure-ml.png) -->\n",
    "\n",
    "![Qwen2.5 32B Instruct on Azure AI Foundry](./qwen2.5-azure-ai-foundry.png)\n",
    "\n",
    "For more information, make sure to check [their model card on the Hugging Face Hub](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct/blob/main/README.md).\n",
    "\n",
    "> [!NOTE]\n",
    "> Note that you can select any LLM available on the Hugging Face Hub with the \"Deploy to AzureML\" option enabled, or directly select any of the LLMs available in either the Azure ML or Azure AI Foundry Hub Model Catalog under the \"HuggingFace\" collection (note that for Azure AI Foundry the Hugging Face Collection will only be available for Hub-based projects)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763a3a3a-aae1-43be-a925-0a4e1f5493a3",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0800470a-050e-4948-b0b2-f6bc3a937e09",
   "metadata": {},
   "source": [
    "To run the following example, you will need to comply with the following pre-requisites, alternatively, you can also read more about those in the [Azure Machine Learning Tutorial: Create resources you need to get started](https://learn.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources?view=azureml-api-2).\n",
    "\n",
    "- An Azure account with an active subscription.\n",
    "- The Azure CLI installed and logged in.\n",
    "- The Azure Machine Learning extension for the Azure CLI.\n",
    "- An Azure Resource Group.\n",
    "- A project based on an Azure AI Foundry Hub.\n",
    "\n",
    "For more information, please go through the steps in [Configure Microsoft Azure for Azure AI](https://huggingface.co/docs/microsoft-azure/azure-ai/configure)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d439f949-f482-4ed9-9d66-0d6ae93d5173",
   "metadata": {},
   "source": [
    "## Setup and installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eade47-ccfa-4add-a10f-933e2190f169",
   "metadata": {},
   "source": [
    "In this example, the [Azure Machine Learning SDK for Python](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/ml/azure-ai-ml) will be used to create the endpoint and the deployment, as well as to invoke the deployed API. Along with it, you will also need to install `azure-identity` to authenticate with your Azure credentials via Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3979198-34e9-48f1-99f1-2c49b447a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-ml azure-identity --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026d4738-831e-4de0-a1df-dcacd05db5b8",
   "metadata": {},
   "source": [
    "More information at [Azure Machine Learning SDK for Python](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-ml-readme?view=azure-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351ec9b-3dbc-4227-82b6-0c71d47a6358",
   "metadata": {},
   "source": [
    "Then, for convenience setting the following environment variables is recommended as those will be used along the example for the Azure ML Client, so make sure to update and set those values accordingly as per your Microsoft Azure account and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5670d3e2-acbb-435b-b851-bca960a2c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env LOCATION eastus\n",
    "%env SUBSCRIPTION_ID <YOUR_SUBSCRIPTION_ID>\n",
    "%env RESOURCE_GROUP <YOUR_RESOURCE_GROUP>\n",
    "%env AI_FOUNDRY_HUB_PROJECT <YOUR_AI_FOUNDRY_HUB_PROJECT>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c2f65-1a8b-419e-b206-5e6f5d9a029d",
   "metadata": {},
   "source": [
    "Finally, you also need to define both the endpoint and deployment names, as those will be used throughout the example too:\n",
    "\n",
    "> [!NOTE]\n",
    "> Note that endpoint names must to be globally unique per region i.e., even if you don't have any endpoint named that way running under your subscription, if the name is reserved by another Azure customer, then you won't be able to use the same name. Adding a timestamp or a custom identifier is recommended to prevent running into HTTP 400 validation issues when trying to deploy an endpoint with an already locked / reserved name. Also the endpoint name must be between 3 and 32 characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e071302-9ea1-43ba-a0a9-8431615f3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from uuid import uuid4\n",
    "\n",
    "os.environ[\"ENDPOINT_NAME\"] = f\"qwen-endpoint-{str(uuid4())[:8]}\"\n",
    "os.environ[\"DEPLOYMENT_NAME\"] = f\"qwen-deployment-{str(uuid4())[:8]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd6812-b59d-46f2-af8a-fd39d4ed3f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $ENDPOINT_NAME\n",
    "!echo $DEPLOYMENT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e1c5f-4815-43b7-9227-f9b7c333329d",
   "metadata": {},
   "source": [
    "## Authenticate to Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026488f0-3a44-433e-b84f-76588678ed77",
   "metadata": {},
   "source": [
    "Initially, you need to authenticate into the Azure AI Foundry Hub via Azure ML with the Azure ML Python SDK, which will be later used to deploy `Qwen/Qwen2.5-32B-Instruct` as an Azure ML Managed Online Endpoint in your Azure AI Foundry Hub.\n",
    "\n",
    "> [!NOTE]\n",
    "> On standard Azure ML deployments you'd need to create the `MLClient` using the Azure ML Workspace as the `workspace_name` whereas for Azure AI Foundry, you need to provide the Azure AI Foundry Hub name as the `workspace_name` instead, and that will deploy the endpoint under the Azure AI Foundry too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe4b31f-34b2-4209-ba99-f3ee99ce2c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "client = MLClient(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    subscription_id=os.getenv(\"SUBSCRIPTION_ID\"),\n",
    "    resource_group_name=os.getenv(\"RESOURCE_GROUP\"),\n",
    "    workspace_name=os.getenv(\"AI_FOUNDRY_HUB_PROJECT\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2498c42-2e9a-4d0f-9c51-2d9775a92eaf",
   "metadata": {},
   "source": [
    "## Create and Deploy Azure AI Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0adfbfd-ca52-4d8c-992b-8d2b72b497d3",
   "metadata": {},
   "source": [
    "Before creating the Managed Online Endpoint, you need to build the model URI, which is formatted as it follows `azureml://registries/<REGISTRY_NAME>/models/<MODEL_ID>/labels/latest` (even if the URI contains `azureml` it's the same as in Azure AI Foundry, since the model catalog is shared), that means that the `REGISTRY_NAME` should be set to \"HuggingFace\" as you intend to deploy a model from the Hugging Face Collection, and the `MODEL_ID` won't be the Hugging Face Hub ID, but rather the ID with hyphen replacements for both backslash (/) and underscores (_) with hyphens (-), and then into lower case, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a77a0-3ce3-418d-b161-757a2b09232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "\n",
    "model_uri = f\"azureml://registries/HuggingFace/models/{model_id.replace('/', '-').replace('_', '-').lower()}/labels/latest\"\n",
    "model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a432367e-e39a-409e-845d-9ea38406359d",
   "metadata": {},
   "source": [
    "Note that you will need to verify in advance that the URI is valid, and that the given Hugging Face Hub Model ID exists on Azure, since Hugging Face is publishing those models into their collection, meaning that some models may be available on the Hugging Face Hub but not yet on the Azure Model Catalog (you can request adding a model following the guide [Request a model addition](https://huggingface.co/docs/microsoft-azure/guides/request-model-addition)).\n",
    "\n",
    "Alternatively, you can use the following snippet to verify if a model is available on the Azure Model Catalog programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01768a4-c860-497e-bdea-1e9528b09469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(f\"https://generate-azureml-urls.azurewebsites.net/api/generate?modelId={model_id}\")\n",
    "if response.status_code != 200:\n",
    "    print(\"[{response.status_code=}] {model_id=} not available on the Hugging Face Collection in Azure Model Catalog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287f007e-b62e-449b-818c-cb35a178aa50",
   "metadata": {},
   "source": [
    "Then you can create the Managed Online Endpoint specifying its name (note that the name must be unique per entire region, not only within a single subscription, resource group, workspace, etc., so it's a nice practice to add some sort of unique name to it in case multi-region deployments are intended) via the [ManagedOnlineEndpoint Python class](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.managedonlineendpoint?view=azure-python).\n",
    "\n",
    "Also note that by default the `ManagedOnlineEndpoint` will use the `key` authentication method, meaning that there will be a primary and secondary key that should be sent within the Authentication headers as a Bearer token; but also the `aml_token` authentication method can be used, read more about it at [Authenticate clients for online endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-authenticate-online-endpoint).\n",
    "\n",
    "The deployment, created via the [ManagedOnlineDeployment Python class](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.managedonlinedeployment?view=azure-python), will define the actual model deployment that will be exposed via the previously created endpoint. The `ManagedOnlineDeployment` will expect: the `model` i.e., the previously created URI `azureml://registries/HuggingFace/models/qwen-qwen2.5-32b-instruct/labels/latest`, the `endpoint_name`, and the instance requirements being the `instance_type` and the `instance_count`.\n",
    "\n",
    "Every model in the Hugging Face Collection is powered by an efficient inference backend, and each of those can run on a wide variety of instance types (as listed in [Supported Hardware](https://huggingface.co/docs/microsoft-azure/azure-ai/supported-hardware)); in this case, a NVIDIA H100 GPU will be used i.e., `Standard_NC40ads_H100_v5`.\n",
    "\n",
    "> [!WARNING]\n",
    "> Since for some models and inference engines you need to run those on a GPU-accelerated instance, you may need to request a quota increase for some of the supported instances as per the model you want to deploy. Also, keep into consideration that each model comes with a list of all the supported instances, being the recommended one for each tier the lower instance in terms of available VRAM. Read more about quota increase requests for Azure ML at [Manage and increase quotas and limits for resources with Azure Machine Learning](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-quotas?view=azureml-api-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e8cd42-5ea4-44a7-b998-dbd1cbf9c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment\n",
    "\n",
    "endpoint = ManagedOnlineEndpoint(name=os.getenv(\"ENDPOINT_NAME\"))\n",
    "\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    name=os.getenv(\"DEPLOYMENT_NAME\"),\n",
    "    endpoint_name=os.getenv(\"ENDPOINT_NAME\"),\n",
    "    model=model_uri,\n",
    "    instance_type=\"Standard_NC40ads_H100_v5\",\n",
    "    instance_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d81645-0a8b-4388-b250-13564ea86b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.begin_create_or_update(endpoint).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646eb1ae-1497-4303-9631-e5bab3022301",
   "metadata": {},
   "source": [
    "<!-- ![Azure AI Endpoint from Azure ML Studio](./azure-ml-endpoint.png) -->\n",
    "\n",
    "![Azure AI Endpoint from Azure AI Foundry](./azure-ai-endpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce20ded-35ee-49f8-a8bd-1f6fd25db162",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> In Azure AI Foundry the endpoint will only be listed within the \"My assets -> Models + endpoints\" tab once the deployment is created, not before as in Azure ML where the endpoint is shown even if it doesn't contain any active or in-progress deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e82cd-3bd4-4ecf-a5bf-6423b37733ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.online_deployments.begin_create_or_update(deployment).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b135caa-a700-4e5c-8f2b-48b9a709c8b2",
   "metadata": {},
   "source": [
    "<!-- ![Azure AI Deployment from Azure ML Studio](./azure-ml-deployment.png) -->\n",
    "\n",
    "![Azure AI Deployment from Azure AI Foundry](./azure-ai-deployment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54884da-0413-438c-8f76-b22302149e4d",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> Note that whilst the Azure AI Endpoint creation is relatively fast, the deployment will take longer since it needs to allocate the resources on Azure so expect it to take ~10-15 minutes, but it could as well take longer depending on the instance provisioning and availability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4cbaca-2fdb-4924-89bf-b3a6c54c998b",
   "metadata": {},
   "source": [
    "Once deployed, via either the Azure AI Foundry or the Azure ML Studio you'll be able to inspect the endpoint details, the real-time logs, how to consume the endpoint, and even use the, still on preview, [monitoring feature](https://learn.microsoft.com/en-us/azure/machine-learning/concept-model-monitoring?view=azureml-api-2).\n",
    "\n",
    "Find more information about it at [Azure ML Managed Online Endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints-online?view=azureml-api-2#managed-online-endpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614189ee-a224-404a-bd5a-2347d7404c5f",
   "metadata": {},
   "source": [
    "## Send requests to the Azure AI Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9b306-7331-4fc3-bb65-d7e2134e7d5e",
   "metadata": {},
   "source": [
    "Finally, now that the Azure AI Endpoint is deployed, you can send requests to it. In this case, since the task of the model is `text-generation` (also known as `chat-completion`) you can either use the default scoring endpoint, being `/generate` which is the standard text generation endpoint without chat capabilities (as leveraging the chat template or having an OpenAI-compatible OpenAPI interface), or alternatively just benefit from the fact that Text Generation Inference (TGI) i.e., the inference engine in which the model is running on top, exposes OpenAI-compatible routes as `/v1/chat/completions`.\n",
    "\n",
    "> [!NOTE]\n",
    "> Note that below only some of the options are listed, but you can send requests to the deployed endpoint as long as you send the HTTP requests with the `azureml-model-deployment` header set to the name of the Azure AI Deployment (not the Endpoint), and have the necessary authentication token / key to send requests to the given endpoint; then you can send HTTP request to all the routes that the backend engine is exposing, not only to the scoring route.\n",
    "\n",
    "> [!WARNING]\n",
    "> Support for Hugging Face models via [`azure-ai-inference` Python SDK](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/ai/azure-ai-inference) is still a work in progress, but that will be included soon and set as the recommended inference method, stay tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8fc287-e86c-4a16-b9fa-82f8db2509fa",
   "metadata": {},
   "source": [
    "### Azure Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de6826-90fd-4ccf-9c95-cf7197d34afb",
   "metadata": {},
   "source": [
    "You can invoke the Azure AI Endpoint on the scoring route, in this case `/generate` (more information about it in the `Qwen/Qwen2.5-32B-Instruct` page in either [AzureML](https://ml.azure.com/models/qwen-qwen2.5-32b-instruct/version/1/catalog/registry/HuggingFace) or [Azure AI Foundry](https://ai.azure.com/explore/models/qwen-qwen2.5-32b-instruct/version/1/registry/HuggingFace) catalogs), via the Azure Python SDK with the previously instantiated `azure.ai.ml.MLClient` (or instantiate a new one if working from a different session)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99c103b-f39e-433d-af15-5cb9776458b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(mode=\"w+\", delete=True, suffix=\".json\") as tmp:\n",
    "    json.dump({\"inputs\": \"What is Deep Learning?\", \"parameters\": {\"max_new_tokens\": 128}}, tmp)\n",
    "    tmp.flush()\n",
    "\n",
    "    response = client.online_endpoints.invoke(\n",
    "        endpoint_name=os.getenv(\"ENDPOINT_NAME\"),\n",
    "        deployment_name=os.getenv(\"DEPLOYMENT_NAME\"),\n",
    "        request_file=tmp.name,\n",
    "    )\n",
    "\n",
    "print(json.loads(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e4439-b590-4f3b-ad84-62c2751ed42b",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> Note that the Azure ML Python SDK requires a path to a JSON file when invoking the endpoints, meaning that whatever payload you want to send to the endpoint will need to be first converted into a JSON file, whilst that only applies to the requests sent via the Azure ML Python SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d6e4b5-9f99-49cd-9b2a-09d8bb1e69bd",
   "metadata": {},
   "source": [
    "### OpenAI Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d470af7-0fee-4e20-97a4-e64a7e8f4071",
   "metadata": {},
   "source": [
    "Since Text Generation Inference (TGI) also exposes OpenAI-compatible routes, you can also leverage the OpenAI Python SDK to send requests to the deployed Azure AI Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37703050-df09-4825-9328-b591ef53632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c2bfad-a1e8-483f-888b-2793097fb01a",
   "metadata": {},
   "source": [
    "To use the OpenAI Python SDK with Azure ML Managed Online Endpoints, you need to first retrieve:\n",
    "\n",
    "- `api_url` with the `/v1` route (that contains the `v1/chat/completions` endpoint that the OpenAI Python SDK will send requests to)\n",
    "- `api_key` which is the API Key in Azure AI or the primary key in Azure ML (unless a dedicated Azure ML Token is used instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dddbe29-8498-46d5-bc71-6903cf8a36ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = client.online_endpoints.get_keys(os.getenv(\"ENDPOINT_NAME\")).primary_key\n",
    "api_url = client.online_endpoints.get(os.getenv(\"ENDPOINT_NAME\")).scoring_uri.replace(\"/generate\", \"/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29679b34-3982-4ed6-92a5-6136d596310d",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> Alternatively, you can also build the API URL manually as it follows, since the URIs are globally unique per region, meaning that there will only be one endpoint named the same way within the same region:\n",
    "> ```python\n",
    "> api_url = f\"https://{os.getenv('ENDPOINT_NAME')}.{os.getenv('LOCATION')}.inference.ml.azure.com/v1\"\n",
    "> ```\n",
    "> Or just retrieve it from either the Azure AI Foundry or the Azure ML Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1e2ff6-c135-43c2-b466-49c9054030ef",
   "metadata": {},
   "source": [
    "Then you can use the OpenAI Python SDK normally, making sure to include the extra header `azureml-model-deployment` header that contains the Azure AI / ML Deployment name.\n",
    "\n",
    "Via the OpenAI Python SDK it can either be set within each call to `chat.completions.create` via the `extra_headers` parameter as commented below, or via the `default_headers` parameter when instantiating the `OpenAI` client (which is the recommended approach since the header needs to be present on each request, so setting it just once is preferred)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa18d5-04f5-4876-820b-d416e91e588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI(\n",
    "    base_url=api_url,\n",
    "    api_key=api_key,\n",
    "    default_headers={\"azureml-model-deployment\": os.getenv(\"DEPLOYMENT_NAME\")},\n",
    ")\n",
    "\n",
    "completion = openai_client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-32B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that responds like a pirate.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is Deep Learning?\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=128,\n",
    "    # extra_headers={\"azureml-model-deployment\": os.getenv(\"DEPLOYMENT_NAME\")},\n",
    ")\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a2463a-b968-4f35-8553-98c3e507123d",
   "metadata": {},
   "source": [
    "### cURL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f92a80-0001-445f-9e07-c00643721a79",
   "metadata": {},
   "source": [
    "Alternatively, you can also just use `cURL` to send requests to the deployed endpoint, with the `api_url` and `api_key` values programmatically retrieved in the OpenAI snippet and now set as environment variables so that `cURL` can use those, as it follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccea9fb-3749-4e9b-ac3b-51304d57fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"API_URL\"] = api_url\n",
    "os.environ[\"API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d88e895-61fc-4b56-831a-76c8ca9ece39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -sS $API_URL/chat/completions \\\n",
    "    -H \"Authorization: Bearer $API_KEY\" \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -H \"azureml-model-deployment: $DEPLOYMENT_NAME\" \\\n",
    "    -d '{ \\\n",
    "\"messages\":[ \\\n",
    "    {\"role\":\"system\",\"content\":\"You are an assistant that replies like a pirate.\"}, \\\n",
    "    {\"role\":\"user\",\"content\":\"What is Deep Learning?\"} \\\n",
    "], \\\n",
    "\"max_tokens\":128 \\\n",
    "}' | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464c9356-be10-4ec8-aa48-195245c2de82",
   "metadata": {},
   "source": [
    "You can also just go to the Azure AI Endpoint in either the Azure AI Foundry under \"My assets -> Models + endpoints\" or in the Azure ML Studio via \"Endpoints\", and retrieve both the URL (note that it will default to the `/generate` endpoint, but to use the OpenAI-compatible layer you need to use the `/v1/chat/completions` endpoint instead) and the API Key values, as well as the Azure AI Deployment name for the given model, and then send the request as follows after replacing the values from Azure ML:\n",
    "\n",
    "```bash\n",
    "curl -sS <API_URL>/v1/chat/completions \\\n",
    "    -H \"Authorization: Bearer <PRIMARY_KEY>\" \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -H \"azureml-model-deployment: $DEPLOYMENT_NAME\" \\\n",
    "    -d '{ \\\n",
    "\"messages\":[ \\\n",
    "    {\"role\":\"system\",\"content\":\"You are an assistant that replies like a pirate.\"}, \\\n",
    "    {\"role\":\"user\",\"content\":\"What is Deep Learning?\"} \\\n",
    "], \\\n",
    "\"max_tokens\":128 \\\n",
    "}' | jq\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5940423f-22fb-4c5a-98ef-852634d1e2bf",
   "metadata": {},
   "source": [
    "### Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0c194-dd5b-4ea6-b984-566aea47f8d2",
   "metadata": {},
   "source": [
    "[Gradio](https://www.gradio.app/) is the fastest way to demo your machine learning model with a friendly web interface so that anyone can use it. You can also leverage the OpenAI Python SDK to build a simple `ChatInterface` that you can use within the Jupyter Notebook cell where you are running it.\n",
    "\n",
    "> [!NOTE]\n",
    "> Ideally you could deploy the Gradio Chat Interface connected to your Azure ML Managed Online Endpoint as an Azure Container App as described in [Tutorial: Build and deploy from source code to Azure Container Apps](https://learn.microsoft.com/en-us/azure/container-apps/tutorial-deploy-from-code?tabs=python). If you'd like us to show you how to do it for Gradio in particular, feel free to [open an issue requesting it](https://github.com/huggingface/Microsoft-Azure/issues/new)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5ae688-cdb5-4751-aa8f-ca2178668185",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gradio --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849062e1-fc55-4e1e-8c0b-88daea23e76f",
   "metadata": {},
   "source": [
    "See below an example on how to leverage Gradio's `ChatInterface`, or find more information about it at [Gradio ChatInterface Docs](https://www.gradio.app/docs/gradio/chatinterface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377ad7c-ecd7-4e98-9910-b4afc5dc393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, Iterator, List, Literal\n",
    "\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI(\n",
    "    base_url=api_url,\n",
    "    api_key=api_key,\n",
    "    default_headers={\"azureml-model-deployment\": os.getenv(\"DEPLOYMENT_NAME\")},\n",
    ")\n",
    "\n",
    "def predict(message: str, history: List[Dict[Literal[\"role\", \"content\"], str]]) -> Iterator[str]:\n",
    "    history.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    stream = openai_client.chat.completions.create(\n",
    "        model=\"Qwen/Qwen2.5-32B-Instruct\",\n",
    "        messages=history,\n",
    "        stream=True,\n",
    "    )\n",
    "    chunks = []\n",
    "    for chunk in stream:\n",
    "        chunks.append(chunk.choices[0].delta.content or \"\")\n",
    "        yield \"\".join(chunks)\n",
    "\n",
    "demo = gr.ChatInterface(predict, type=\"messages\")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b5549e-345f-4b21-88c2-e1019a3f30b4",
   "metadata": {},
   "source": [
    "![Gradio Chat Interface with Azure AI Endpoint](./azure-ml-gradio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf20842-e691-4f8c-949c-fb3fc1038336",
   "metadata": {},
   "source": [
    "## Release resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c475ca0a-cea2-45bc-b93c-3824fc73cf6d",
   "metadata": {},
   "source": [
    "Once you are done using the Azure AI Endpoint / Deployment, you can delete the resources as it follows, meaning that you will stop paying for the instance on which the model is running and all the attached costs will be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd52337-cce6-431c-b036-fb91d0a894ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.online_endpoints.begin_delete(name=os.getenv(\"ENDPOINT_NAME\")).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa2816f-bae1-48e4-8b4d-753b34a149ac",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d01c0b-44fc-40b6-bfc7-7ba8dcb1432f",
   "metadata": {},
   "source": [
    "Throughout this example you learnt how to create and configure your Azure account for Azure ML and Azure AI Foundry, how to then create a Managed Online Endpoint running an open model from the Hugging Face Collection in the Azure ML / Azure AI Foundry model catalog, how to send inference requests to it afterwards with different alternatives, how to build a simple Gradio chat interface around it, and finally, how to stop and release the resources.\n",
    "\n",
    "If you have any doubt, issue or question about this example, feel free to [open an issue](https://github.com/huggingface/Microsoft-Azure/issues/new) and we'll do our best to help!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
